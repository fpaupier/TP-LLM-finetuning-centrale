{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Partie 2 : Fine-tuning de LLM avec LoRA\n",
    "\n",
    "## üìö Contexte\n",
    "\n",
    "Dans ce notebook, nous allons explorer le fine-tuning de mod√®les de langage avec la technique LoRA (Low-Rank Adaptation). Cette approche permet d'adapter des LLMs pour des t√¢ches sp√©cifiques.\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "1. **Comprendre** le fonctionnement de LoRA et ses avantages\n",
    "2. **Pr√©parer** le dataset synth√©tique g√©n√©r√© pr√©c√©demment\n",
    "3. **Configurer** un entra√Ænement LoRA avec Unsloth\n",
    "4. **Entra√Æner** un mod√®le Qwen3-4B sp√©cialis√© en fran√ßais\n",
    "5. **√âvaluer** les performances du mod√®le fine-tun√©\n",
    "6. **Publier** le mod√®le sur Hugging Face Hub\n",
    "\n",
    "## üîß Etapes\n",
    "\n",
    "Notre approche utilise :\n",
    "\n",
    "1. **Unsloth** : Biblioth√®que optimis√©e pour le fine-tuning LoRA\n",
    "2. **Qwen3-4B** : Mod√®le de base pr√©-entra√Æn√©\n",
    "3. **QLoRA** : Quantization + LoRA pour r√©duire la m√©moire GPU\n",
    "4. **Hugging Face Hub** : Publication et partage du mod√®le\n",
    "\n",
    "Resources √† garder sous la main durant le TP:\n",
    "\n",
    "- [LoRA Hyperparameters Guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide)\n",
    "- [What Model Should I Use?](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/what-model-should-i-use)\n",
    "- [Qwen3 instruct fine tune guide](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-Instruct.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T14:04:43.815441Z",
     "start_time": "2025-09-02T14:04:43.807337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Toutes les biblioth√®ques ont √©t√© import√©es avec succ√®s!\n",
      "Device utilis√©: cuda\n",
      "GPU: NVIDIA L4\n",
      "M√©moire GPU: 23.7 GB\n"
     ]
    }
   ],
   "source": [
    "# Import des biblioth√®ques\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Imports Unsloth\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    from trl import SFTTrainer\n",
    "    import trackio\n",
    "    \n",
    "    print(\"‚úÖ Toutes les biblioth√®ques ont √©t√© import√©es avec succ√®s!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erreur d'import: {e}\")\n",
    "    print(\"Veuillez ex√©cuter 'uv sync' pour installer les d√©pendances.\")\n",
    "    raise\n",
    "\n",
    "# V√©rification GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device utilis√©: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "# Configuration visuelle\n",
    "sns.set_theme()\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement et pr√©paration du dataset\n",
    "\n",
    "Nous chargeons le dataset synth√©tique g√©n√©r√© dans la partie pr√©c√©dente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset synth√©tique...\n",
      "‚úÖ Dataset charg√©: 36 paires instruction-r√©ponse\n",
      "\n",
      "üìä Statistiques du dataset:\n",
      "   - Nombre total de paires: 36\n",
      "   - Longueur moyenne des instructions: 102 caract√®res\n",
      "   - Longueur moyenne des r√©ponses: 919 caract√®res\n",
      "\n",
      "üìù Exemples de donn√©es:\n",
      "\n",
      "--- Exemple 1 ---\n",
      "Instruction: En quelle ann√©e a eu lieu la consultation de l'√©lection √©tatique de Katanning ?\n",
      "R√©ponse: La consultation de l'√©lection √©tatique de Katanning a eu lieu en **1935**....\n",
      "\n",
      "--- Exemple 2 ---\n",
      "Instruction: Quel est le nom du cat√©gory de Wikimedia qui regroupe les icefalls de la r√©gion de Ross Dependency ?\n",
      "R√©ponse: Le nom du cat√©gory de Wikimedia qui regroupe les icefalls de la r√©gion de Ross Dependency est **Cate...\n",
      "\n",
      "--- Exemple 3 ---\n",
      "Instruction: Dans quel film Bourvil joue-t-il son dernier r√¥le, selon Jean Pierre Melville, et en quelle ann√©e a-t-il √©t√© tourn√© ?\n",
      "R√©ponse: Selon Jean Pierre Melville, Bourvil joue son dernier r√¥le dans le film **\"Le cercle rouge\"**, qui a ...\n"
     ]
    }
   ],
   "source": [
    "# Configuration des chemins\n",
    "data_dir = Path(\"../data/synthetic\")\n",
    "\n",
    "# Chargement du dataset\n",
    "print(\"Chargement du dataset synth√©tique...\")\n",
    "if (data_dir / \"synthetic_dataset_alpaca.json\").exists():\n",
    "    with open(data_dir / \"synthetic_dataset_alpaca.json\", 'r', encoding='utf-8') as f:\n",
    "        alpaca_data = json.load(f)\n",
    "    print(f\"‚úÖ Dataset charg√©: {len(alpaca_data)} paires instruction-r√©ponse\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset non trouv√©. Veuillez ex√©cuter le notebook 01 d'abord.\")\n",
    "    # Cr√©ation d'un dataset d'exemple\n",
    "    alpaca_data = [\n",
    "        {\n",
    "            \"instruction\": \"Qu'est-ce que l'apprentissage automatique?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"L'apprentissage automatique est un sous-domaine de l'intelligence artificielle qui permet aux syst√®mes d'apprendre √† partir de donn√©es sans √™tre explicitement programm√©s.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Conversion en DataFrame pour analyse\n",
    "df = pd.DataFrame(alpaca_data)\n",
    "print(f\"\\nüìä Statistiques du dataset:\")\n",
    "print(f\"   - Nombre total de paires: {len(df)}\")\n",
    "print(f\"   - Longueur moyenne des instructions: {df['instruction'].str.len().mean():.0f} caract√®res\")\n",
    "print(f\"   - Longueur moyenne des r√©ponses: {df['output'].str.len().mean():.0f} caract√®res\")\n",
    "\n",
    "# Affichage des premi√®res lignes\n",
    "print(\"\\nüìù Exemples de donn√©es:\")\n",
    "for i in range(min(3, len(alpaca_data))):\n",
    "    print(f\"\\n--- Exemple {i+1} ---\")\n",
    "    print(f\"Instruction: {alpaca_data[i]['instruction']}\")\n",
    "    print(f\"R√©ponse: {alpaca_data[i]['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pr√©paration des donn√©es pour l'entra√Ænement\n",
    "\n",
    "Nous formatons les donn√©es selon le format attendu par Unsloth pour le fine-tuning instructionnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatage des donn√©es pour l'entra√Ænement...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4040421b0af54e31b7d5f201bebdc8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset pr√©par√©:\n",
      "   - Training set: 32 exemples\n",
      "   - Validation set: 4 exemples\n",
      "\n",
      "üìù Exemple format√©:\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "O√π se trouve le Barnes Ridge et dans quel √©tat est-il situ√© ?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Le Barnes Ridge se trouve dans le **state de Montana**, aux √âtats-Unis. Il est situ√© dans le **comt√© de Fergus County**, dans la partie centrale du pays, √† environ 300 km au sud-ouest de Washington, D.C.<|im_end|>...\n"
     ]
    }
   ],
   "source": [
    "# Template de formatage pour le mod√®le - align√© avec le format Unsloth/Qwen3\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# Le EOS_TOKEN sera d√©fini apr√®s le chargement du tokenizer\n",
    "EOS_TOKEN = \"<|im_end|>\"  # Token de fin pour Qwen3\n",
    "\n",
    "def format_prompts(examples):\n",
    "    \"\"\"\n",
    "    Formate les exemples selon le template Alpaca pour le fine-tuning.\n",
    "    \"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    \n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        # Formatage du prompt\n",
    "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Conversion en Dataset Hugging Face\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Application du formatage\n",
    "print(\"Formatage des donn√©es pour l'entra√Ænement...\")\n",
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "\n",
    "# Split train/validation\n",
    "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test']\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset pr√©par√©:\")\n",
    "print(f\"   - Training set: {len(dataset_dict['train'])} exemples\")\n",
    "print(f\"   - Validation set: {len(dataset_dict['validation'])} exemples\")\n",
    "\n",
    "# Affichage d'un exemple format√©\n",
    "print(\"\\nüìù Exemple format√©:\")\n",
    "print(dataset_dict['train'][0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration du mod√®le et de LoRA avec TrackIO\n",
    "\n",
    "Nous configurons le mod√®le Qwen3-4B avec les param√®tres LoRA optimis√©s selon les meilleures pratiques Unsloth, et initialisons TrackIO pour le suivi des exp√©riences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä TrackIO initialis√© - Exp√©rience: qwen3-4b-lora-french-1756829193\n",
      "Chargement du mod√®le Qwen/Qwen3-4B-Instruct-2507...\n",
      "==((====))==  Unsloth 2025.8.10: Fast Qwen3 patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.045 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf5d62b8a69425aade3901378f990ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7039956c44764bb4a76fb32e0a13e90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d403b9b433614c9eae08951ac8efb4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30f3390a0b74a828ad0beeec3735382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba5ff39990647d4beed29f239a9b3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9caf04027c14040968a089c3ea79efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50e000e35284aaa83981e97349d847b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3cb3d9cd294dc495c518ef1118e4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le charg√© avec succ√®s!\n",
      "   - M√©moire utilis√©e: 3.6 GB\n",
      "   - M√©moire r√©serv√©e: 3.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Initialisation de TrackIO pour le suivi des exp√©riences\n",
    "experiment_name = f\"qwen3-4b-lora-french-{int(time.time())}\"\n",
    "trackio.init(\n",
    "    project=\"llm-finetuning-tp\",\n",
    "    name=experiment_name,\n",
    "    config={\n",
    "        \"model\": \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "        \"dataset\": \"synthetic_french\",\n",
    "        \"framework\": \"unsloth\",\n",
    "        \"optimization\": \"qlora\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"üìä TrackIO initialis√© - Exp√©rience: {experiment_name}\")\n",
    "\n",
    "# Configuration du mod√®le avec param√®tres optimis√©s Unsloth\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "max_seq_length = 2048  # Longueur maximale de s√©quence\n",
    "dtype = None  # D√©tection automatique du type\n",
    "load_in_4bit = True  # Utilisation de la quantification 4-bit\n",
    "\n",
    "print(f\"Chargement du mod√®le {model_name}...\")\n",
    "\n",
    "# Chargement du mod√®le avec Unsloth\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    token=None,  # Pas besoin de token pour les mod√®les publics\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© avec succ√®s!\")\n",
    "print(f\"   - M√©moire utilis√©e: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "print(f\"   - M√©moire r√©serv√©e: {torch.cuda.memory_reserved() / 1e9:.1f} GB\")\n",
    "\n",
    "# Mettre √† jour le EOS_TOKEN avec le tokenizer r√©el\n",
    "global EOS_TOKEN\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Log des informations syst√®me\n",
    "trackio.log({\n",
    "    \"gpu_memory_allocated_gb\": torch.cuda.memory_allocated() / 1e9,\n",
    "    \"gpu_memory_reserved_gb\": torch.cuda.memory_reserved() / 1e9,\n",
    "    \"model_parameters_total\": sum(p.numel() for p in model.parameters()),\n",
    "    \"model_size_gb\": sum(p.numel() * p.element_size() for p in model.parameters()) / 1e9\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration LoRA:\n",
      "   - r: 16\n",
      "   - target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "   - lora_alpha: 16\n",
      "   - lora_dropout: 0\n",
      "   - bias: none\n",
      "   - use_gradient_checkpointing: unsloth\n",
      "   - random_state: 3407\n",
      "   - use_rslora: False\n",
      "   - loftq_config: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.10 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Configuration LoRA appliqu√©e!\n",
      "trainable params: 33,030,144 || all params: 4,055,498,240 || trainable%: 0.8145\n",
      "   - Param√®tres entra√Ænables: None\n"
     ]
    }
   ],
   "source": [
    "# Configuration des param√®tres LoRA optimis√©s selon les best practuces Unsloth\n",
    "# Bas√© sur: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-Instruct.ipynb\n",
    "lora_config = {\n",
    "    \"r\": 16,  # Rank des matrices de mise √† jour\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],  # Modules √† adapter - optimis√© pour Qwen3\n",
    "    \"lora_alpha\": 16,  # Facteur d'√©chelle (align√© avec le rank)\n",
    "    \"lora_dropout\": 0,  # Pas de dropout pour une meilleure stabilit√©\n",
    "    \"bias\": \"none\",  # Pas d'adaptation des biais\n",
    "    \"use_gradient_checkpointing\": \"unsloth\",  # Optimis√© Unsloth\n",
    "    \"random_state\": 3407,\n",
    "    \"use_rslora\": False,  # Ne pas utiliser scaled LoRA\n",
    "    \"loftq_config\": None,  # Configuration LoftQ (pour quantification avanc√©e)\n",
    "}\n",
    "\n",
    "print(\"Configuration LoRA:\")\n",
    "for key, value in lora_config.items():\n",
    "    print(f\"   - {key}: {value}\")\n",
    "\n",
    "# Application de la configuration LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    **lora_config\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Configuration LoRA appliqu√©e!\")\n",
    "\n",
    "# Log des param√®tres LoRA\n",
    "trackio.log({\n",
    "    \"lora_rank\": lora_config[\"r\"],\n",
    "    \"lora_alpha\": lora_config[\"lora_alpha\"],\n",
    "    \"lora_dropout\": lora_config[\"lora_dropout\"],\n",
    "    \"target_modules\": lora_config[\"target_modules\"],\n",
    "    \"gradient_checkpointing\": lora_config[\"use_gradient_checkpointing\"]\n",
    "})\n",
    "\n",
    "# Affichage des param√®tres entra√Ænables\n",
    "trainable_stats = model.print_trainable_parameters()\n",
    "print(f\"   - Param√®tres entra√Ænables: {trainable_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration de l'entra√Ænement\n",
    "\n",
    "Nous configurons les hyperparam√®tres d'entra√Ænement et le trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments d'entra√Ænement:\n",
      "   - Batch size: 2\n",
      "   - Gradient accumulation: 4\n",
      "   - Learning rate: 0.0002\n",
      "   - Epochs: 3\n",
      "   - Warmup steps: 5\n",
      "   - Report to: ['trackio']\n"
     ]
    }
   ],
   "source": [
    "# Configuration des arguments d'entra√Ænement optimis√©s avec TrackIO\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetuned_model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    num_train_epochs=3,  # Nombre d'√©poques\n",
    "    learning_rate=2e-4,  # Taux d'apprentissage optimal pour LoRA\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    report_to=\"trackio\",  # Utilisation de TrackIO pour le tracking\n",
    "    # Param√®tres suppl√©mentaires pour le monitoring\n",
    "    logging_first_step=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    ddp_find_unused_parameters=False,\n",
    ")\n",
    "\n",
    "print(\"Arguments d'entra√Ænement:\")\n",
    "print(f\"   - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   - Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   - Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"   - Report to: {training_args.report_to}\")\n",
    "\n",
    "# Log des hyperparam√®tres\n",
    "trackio.log({\n",
    "    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"num_train_epochs\": training_args.num_train_epochs,\n",
    "    \"warmup_steps\": training_args.warmup_steps,\n",
    "    \"weight_decay\": training_args.weight_decay,\n",
    "    \"optimizer\": training_args.optim,\n",
    "    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n",
    "    \"effective_batch_size\": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41ffc6604284afba6163a7879fb18b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=12):   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n",
      "WARNING:datasets.arrow_dataset:num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d919ceb63714d70aa43dd2af29bd446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SFTTrainer configur√©!\n",
      "   - √âchantillons d'entra√Ænement: 32\n",
      "   - √âchantillons de validation: 4\n",
      "   - Sequence length: 2048\n",
      "   - Packing: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration du trainer SFT de TRL\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"validation\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Ne pas packer les s√©quences pour Qwen3\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SFTTrainer configur√©!\")\n",
    "print(f\"   - √âchantillons d'entra√Ænement: {len(trainer.train_dataset)}\")\n",
    "print(f\"   - √âchantillons de validation: {len(trainer.eval_dataset)}\")\n",
    "print(f\"   - Sequence length: {max_seq_length}\")\n",
    "print(f\"   - Packing: {False}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entra√Ænement du mod√®le\n",
    "\n",
    "**‚ö†Ô∏è Important** : Cette √©tape peut prendre du temps (15-30 minutes) selon votre GPU et la taille du dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©marrage de l'entra√Ænement LoRA...\n",
      "   - Dataset: 32 exemples\n",
      "   - Param√®tres LoRA: rank=16, alpha=16\n",
      "   - Learning rate: 0.0002\n",
      "   - Epochs: 3\n",
      "   - TrackIO: Activ√©\\n\n",
      "M√©moire GPU avant entra√Ænement: 3.7 GB\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "pynvml does not seem to be installed or it can't be imported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m         trackio.log(gpu_stats)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Log initial\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mlog_gpu_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m trackio.log({\n\u001b[32m     34\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtraining_start_time\u001b[39m\u001b[33m\"\u001b[39m: time.time(),\n\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdataset_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(dataset_dict[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvalidation_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(dataset_dict[\u001b[33m'\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     37\u001b[39m })\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Lancement de l'entra√Ænement\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mlog_gpu_stats\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_gpu_stats\u001b[39m():\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m     16\u001b[39m         gpu_stats = {\n\u001b[32m     17\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgpu_memory_allocated_gb\u001b[39m\u001b[33m\"\u001b[39m: torch.cuda.memory_allocated() / \u001b[32m1e9\u001b[39m,\n\u001b[32m     18\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgpu_memory_reserved_gb\u001b[39m\u001b[33m\"\u001b[39m: torch.cuda.memory_reserved() / \u001b[32m1e9\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgpu_utilization_percent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutilization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     20\u001b[39m         }\n\u001b[32m     21\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     22\u001b[39m             \u001b[38;5;66;03m# Tentative de lecture de la temp√©rature et de la puissance\u001b[39;00m\n\u001b[32m     23\u001b[39m             gpu_stats.update({\n\u001b[32m     24\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mgpu_temperature_c\u001b[39m\u001b[33m\"\u001b[39m: torch.cuda.temperature(),\n\u001b[32m     25\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mgpu_power_watts\u001b[39m\u001b[33m\"\u001b[39m: torch.cuda.power_draw(),\n\u001b[32m     26\u001b[39m             })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TP-LLM-finetuning-centrale/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:1360\u001b[39m, in \u001b[36mutilization\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m   1348\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return the percent of time over the past sample period during which one or\u001b[39;00m\n\u001b[32m   1349\u001b[39m \u001b[33;03mmore kernels was executing on the GPU as given by `nvidia-smi`.\u001b[39;00m\n\u001b[32m   1350\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1357\u001b[39m \u001b[33;03mdepending on the product being queried.\u001b[39;00m\n\u001b[32m   1358\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.version.hip:\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m     handle = \u001b[43m_get_pynvml_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1361\u001b[39m     device = _get_nvml_device_index(device)\n\u001b[32m   1362\u001b[39m     handle = pynvml.nvmlDeviceGetHandleByIndex(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TP-LLM-finetuning-centrale/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:1204\u001b[39m, in \u001b[36m_get_pynvml_handler\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_pynvml_handler\u001b[39m(device: \u001b[33m\"\u001b[39m\u001b[33mDevice\u001b[39m\u001b[33m\"\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAS_PYNVML:\n\u001b[32m-> \u001b[39m\u001b[32m1204\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[32m   1205\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpynvml does not seem to be installed or it can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be imported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1206\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_PYNVML_ERR\u001b[39;00m\n\u001b[32m   1207\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpynvml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NVMLError_DriverNotLoaded\n\u001b[32m   1209\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: pynvml does not seem to be installed or it can't be imported."
     ]
    }
   ],
   "source": [
    "# D√©marrage de l'entra√Ænement avec monitoring TrackIO\n",
    "print(\"üöÄ D√©marrage de l'entra√Ænement LoRA...\")\n",
    "print(f\"   - Dataset: {len(dataset_dict['train'])} exemples\")\n",
    "print(f\"   - Param√®tres LoRA: rank={lora_config['r']}, alpha={lora_config['lora_alpha']}\")\n",
    "print(f\"   - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   - TrackIO: Activ√©\\\\n\")\n",
    "\n",
    "# Suivi de la m√©moire avant entra√Ænement\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"M√©moire GPU avant entra√Ænement: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "\n",
    "# Fonction de monitoring GPU pour TrackIO\n",
    "def log_gpu_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_stats = {\n",
    "            \"gpu_memory_allocated_gb\": torch.cuda.memory_allocated() / 1e9,\n",
    "            \"gpu_memory_reserved_gb\": torch.cuda.memory_reserved() / 1e9,\n",
    "            \"gpu_utilization_percent\": torch.cuda.utilization(),\n",
    "        }\n",
    "        try:\n",
    "            # Tentative de lecture de la temp√©rature et de la puissance\n",
    "            gpu_stats.update({\n",
    "                \"gpu_temperature_c\": torch.cuda.temperature(),\n",
    "                \"gpu_power_watts\": torch.cuda.power_draw(),\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "        trackio.log(gpu_stats)\n",
    "\n",
    "# Log initial\n",
    "log_gpu_stats()\n",
    "trackio.log({\n",
    "    \"training_start_time\": time.time(),\n",
    "    \"dataset_size\": len(dataset_dict['train']),\n",
    "    \"validation_size\": len(dataset_dict['validation'])\n",
    "})\n",
    "\n",
    "# Lancement de l'entra√Ænement\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\\\n‚úÖ Entra√Ænement termin√©!\")\n",
    "print(f\"   - Perte finale: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"   - Temps d'entra√Ænement: {trainer_stats.metrics.get('train_runtime', 0):.1f} secondes\")\n",
    "\n",
    "# Suivi de la m√©moire apr√®s entra√Ænement\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"M√©moire GPU apr√®s entra√Ænement: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "\n",
    "# Log des r√©sultats finaux\n",
    "log_gpu_stats()\n",
    "trackio.log({\n",
    "    \"training_end_time\": time.time(),\n",
    "    \"final_training_loss\": trainer_stats.training_loss,\n",
    "    \"training_runtime_seconds\": trainer_stats.metrics.get('train_runtime', 0),\n",
    "    \"training_samples_per_second\": trainer_stats.metrics.get('train_samples_per_second', 0),\n",
    "    \"total_steps\": trainer.state.global_step\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. √âvaluation du mod√®le\n",
    "\n",
    "Nous √©valuons les performances du mod√®le fine-tun√© sur le jeu de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation sur le jeu de validation avec TrackIO\n",
    "print(\"üìä √âvaluation du mod√®le sur le jeu de validation...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\\\nR√©sultats de l'√©valuation:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   - {key}: {value:.4f}\")\n",
    "\n",
    "# Log des r√©sultats d'√©valuation\n",
    "trackio.log({\n",
    "    \"eval_loss\": eval_results.get(\"eval_loss\", 0),\n",
    "    \"eval_runtime\": eval_results.get(\"eval_runtime\", 0),\n",
    "    \"eval_samples_per_second\": eval_results.get(\"eval_samples_per_second\", 0),\n",
    "    \"eval_steps_per_second\": eval_results.get(\"eval_steps_per_second\", 0)\n",
    "})\n",
    "\n",
    "# Visualisation de la courbe d'apprentissage\n",
    "if hasattr(trainer, \"state\") and trainer.state.log_history:\n",
    "    log_history = trainer.state.log_history\n",
    "    losses = [log.get('loss', 0) for log in log_history if 'loss' in log]\n",
    "    steps = [log.get('step', 0) for log in log_history if 'loss' in log]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(steps, losses, 'b-', label='Training Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Courbe d\\\\'apprentissage')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Log des m√©triques de la courbe d'apprentissage\n",
    "    if losses:\n",
    "        trackio.log({\n",
    "            \"training_loss_min\": min(losses),\n",
    "            \"training_loss_final\": losses[-1],\n",
    "            \"training_loss_improvement\": losses[0] - losses[-1] if len(losses) > 1 else 0\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test d'inf√©rence\n",
    "\n",
    "Nous testons le mod√®le fine-tun√© sur quelques exemples pour √©valuer sa capacit√© √† g√©n√©rer des r√©ponses en fran√ßais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du mod√®le pour l'inf√©rence optimis√©e avec Unsloth\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Fonction de g√©n√©ration optimis√©e avec Unsloth\n",
    "def generate_response(instruction: str, max_new_tokens: int = 512) -> str:\n",
    "    \"\"\"\n",
    "    G√©n√®re une r√©ponse √† partir d'une instruction avec param√®tres optimis√©s Unsloth.\n",
    "    \"\"\"\n",
    "    # Formatage du prompt selon le template utilis√© pendant l'entra√Ænement\n",
    "    prompt = alpaca_prompt.format(\n",
    "        instruction,\n",
    "        \"\",  # Input vide\n",
    "        \"\"  # Laisser vide pour la g√©n√©ration\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # G√©n√©ration avec param√®tres optimis√©s pour le fran√ßais\n",
    "    # Utilisation des optimisations Unsloth\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,  # Activ√© par d√©faut avec Unsloth\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    # Extraire seulement la r√©ponse g√©n√©r√©e\n",
    "    response = response.split(\"### Response:\")[1].strip()\n",
    "    return response\n",
    "\n",
    "# Test avec exemples vari√©s en fran√ßais\n",
    "test_instructions = [\n",
    "    \"Qu'est-ce que l'intelligence artificielle?\",\n",
    "    \"Explique le principe du fine-tuning avec LoRA.\",\n",
    "    \"Quels sont les avantages de QLoRA par rapport au fine-tuning classique?\",\n",
    "    \"Comment fonctionne l'apprentissage par renforcement?\",\n",
    "    \"Qu'est-ce que la quantification 4-bit dans les LLMs?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Tests d'inf√©rence:\\n\")\n",
    "\n",
    "# Log des tests d'inf√©rence\n",
    "inference_results = []\n",
    "\n",
    "for i, instruction in enumerate(test_instructions):\n",
    "    print(f\"--- Test {i+1} ---\")\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = generate_response(instruction)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"R√©ponse: {response[:300]}...\")\n",
    "    print(f\"Temps de g√©n√©ration: {inference_time:.2f} secondes\\n\")\n",
    "    \n",
    "    # Stocker les r√©sultats pour le logging\n",
    "    inference_results.append({\n",
    "        \"test_id\": i + 1,\n",
    "        \"instruction\": instruction,\n",
    "        \"response_length\": len(response),\n",
    "        \"inference_time\": inference_time,\n",
    "        \"tokens_per_second\": len(response.split()) / inference_time if inference_time > 0 else 0\n",
    "    })\n",
    "\n",
    "# Log des r√©sultats d'inf√©rence\n",
    "trackio.log({\n",
    "    \"inference_tests\": inference_results,\n",
    "    \"avg_inference_time\": sum(r[\"inference_time\"] for r in inference_results) / len(inference_results),\n",
    "    \"avg_tokens_per_second\": sum(r[\"tokens_per_second\"] for r in inference_results) / len(inference_results)\n",
    "})\n",
    "\n",
    "# Afficher les statistiques d'inf√©rence\n",
    "print(\"\\nüìä Statistiques d'inf√©rence:\")\n",
    "print(f\"   - Temps moyen: {sum(r['inference_time'] for r in inference_results) / len(inference_results):.2f}s\")\n",
    "print(f\"   - Tokens/sec moyen: {sum(r['tokens_per_second'] for r in inference_results) / len(inference_results):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyse des compromis LoRA\n",
    "\n",
    "Nous analysons les avantages et inconv√©nients de l'approche LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des param√®tres entra√Ænables\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(\"üìä Analyse des param√®tres:\")\n",
    "print(f\"   - Param√®tres totaux: {total_params:,}\")\n",
    "print(f\"   - Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "print(f\"   - Pourcentage entra√Ænable: {percentage:.2f}%\")\n",
    "\n",
    "# Comparaison des approches\n",
    "comparison_data = {\n",
    "    \"Approche\": [\"Full Fine-tuning\", \"LoRA (r=16)\", \"LoRA (r=8)\", \"LoRA (r=32)\"],\n",
    "    \"Param√®tres entra√Ænables\": [\n",
    "        f\"{total_params:,}\",\n",
    "        f\"{trainable_params:,}\",\n",
    "        f\"{trainable_params//2:,}\",\n",
    "        f\"{trainable_params*2:,}\"\n",
    "    ],\n",
    "    \"M√©moire GPU estim√©e\": [\"~24GB\", \"~8GB\", \"~6GB\", \"~10GB\"],\n",
    "    \"Temps d'entra√Ænement\": [\"100%\", \"~30%\", \"~20%\", \"~40%\"],\n",
    "    \"Performance relative\": [\"100%\", \"~95%\", \"~85%\", \"~98%\"]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüîÑ Comparaison des approches:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution des types de param√®tres\n",
    "param_types = ['Entra√Ænables', 'Gel√©s']\n",
    "param_counts = [trainable_params, total_params - trainable_params]\n",
    "\n",
    "axes[0].pie(param_counts, labels=param_types, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Distribution des param√®tres')\n",
    "\n",
    "# Efficacit√© m√©moire\n",
    "approaches = ['Full FT', 'LoRA r=16', 'LoRA r=8', 'LoRA r=32']\n",
    "memory_usage = [100, 33, 25, 42]  # Pourcentage du full fine-tuning\n",
    "\n",
    "axes[1].bar(approaches, memory_usage, color=['red', 'green', 'blue', 'orange'])\n",
    "axes[1].set_ylabel('M√©moire GPU relative (%)')\n",
    "axes[1].set_title('Efficacit√© m√©moire')\n",
    "axes[1].set_ylim(0, 120)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for i, v in enumerate(memory_usage):\n",
    "    axes[1].text(i, v + 2, f'{v}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sauvegarde du mod√®le\n",
    "\n",
    "Nous sauvegardons le mod√®le fine-tun√© pour une utilisation future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du dossier de sauvegarde\n",
    "model_save_path = Path(\"../models/lora_qwen3_french\")\n",
    "model_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Sauvegarde du mod√®le dans {model_save_path}...\")\n",
    "\n",
    "# Sauvegarde du mod√®le LoRA avec Unsloth\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(\"‚úÖ Adaptateurs LoRA sauvegard√©s avec succ√®s!\")\n",
    "\n",
    "# Sauvegarde de la configuration\n",
    "config = {\n",
    "    \"base_model\": model_name,\n",
    "    \"lora_config\": lora_config,\n",
    "    \"training_args\": {\n",
    "        \"num_train_epochs\": training_args.num_train_epochs,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"train_size\": len(dataset_dict['train']),\n",
    "        \"val_size\": len(dataset_dict['validation']),\n",
    "        \"source\": \"synthetic_dataset\"\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"final_loss\": trainer_stats.training_loss,\n",
    "        \"eval_results\": eval_results\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(model_save_path / \"config.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Sauvegarde en 16-bit pour l'inf√©rence plus rapide\n",
    "print(\"\\nüíæ Sauvegarde en 16-bit pour l'inf√©rence...\")\n",
    "model.save_pretrained_merged(\n",
    "    model_save_path / \"merged_16bit\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\"\n",
    ")\n",
    "print(\"‚úÖ Sauvegarde 16-bit termin√©e!\")\n",
    "\n",
    "# Sauvegarde en 4-bit pour la taille minimale\n",
    "print(\"\\nüíæ Sauvegarde en 4-bit pour la taille minimale...\")\n",
    "model.save_pretrained_merged(\n",
    "    model_save_path / \"merged_4bit\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_4bit_forced\"\n",
    ")\n",
    "print(\"‚úÖ Sauvegarde 4-bit termin√©e!\")\n",
    "\n",
    "# Affichage des tailles des mod√®les\n",
    "print(\"\\nüìä Tailles des mod√®les sauvegard√©s:\")\n",
    "for folder in [\"\", \"merged_16bit\", \"merged_4bit\"]:\n",
    "    folder_path = model_save_path / folder if folder else model_save_path\n",
    "    if folder_path.exists():\n",
    "        size = sum(os.path.getsize(folder_path / f) for f in os.listdir(folder_path) \n",
    "                  if os.path.isfile(folder_path / f)) / 1e6\n",
    "        print(f\"   - {folder if folder else 'LoRA adapters'}: {size:.1f} MB\")\n",
    "\n",
    "# Log des informations de sauvegarde\n",
    "trackio.log({\n",
    "    \"model_saved\": True,\n",
    "    \"save_path\": str(model_save_path),\n",
    "    \"saved_formats\": [\"lora_adapters\", \"merged_16bit\", \"merged_4bit\"],\n",
    "    \"final_training_loss\": trainer_stats.training_loss\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Pr√©paration pour la publication sur Hugging Face\n",
    "\n",
    "Nous pr√©parons le mod√®le pour sa publication sur Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du README pour Hugging Face\n",
    "readme_content = f\"\"\"---\n",
    "library_name: transformers\n",
    "license: mit\n",
    "base_model: {model_name}\n",
    "tags:\n",
    "- trl\n",
    "- sft\n",
    "- generated_from_trainer\n",
    "- french\n",
    "- lora\n",
    "- qwen3\n",
    "model-index:\n",
    "- name: Qwen3-4B-French-LoRA\n",
    "  results: []\n",
    "---\n",
    "\n",
    "# Qwen3-4B-French-LoRA\n",
    "\n",
    "## Description\n",
    "\n",
    "Ce mod√®le est une version fine-tun√©e de Qwen3-4B-Instruct sp√©cialis√©e pour le fran√ßais. Il a √©t√© entra√Æn√© sur un dataset synth√©tique d'instructions-r√©ponses en fran√ßais.\n",
    "\n",
    "## D√©tails d'entra√Ænement\n",
    "\n",
    "- **Base Model**: {model_name}\n",
    "- **Training Framework**: Unsloth + TRL\n",
    "- **Quantization**: 4-bit (QLoRA)\n",
    "- **LoRA Rank**: {lora_config['r']}\n",
    "- **LoRA Alpha**: {lora_config['lora_alpha']}\n",
    "- **Training Epochs**: {training_args.num_train_epochs}\n",
    "- **Learning Rate**: {training_args.learning_rate}\n",
    "- **Dataset Size**: {len(dataset_dict['train'])} examples\n",
    "\n",
    "## Performance\n",
    "\n",
    "- **Training Loss**: {trainer_stats.training_loss:.4f}\n",
    "- **Validation Loss**: {eval_results.get('eval_loss', 'N/A')}\n",
    "\n",
    "## Utilisation\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Chargement du mod√®le\n",
    "model_path = \"votre-username/Qwen3-4B-French-LoRA\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Formatage du prompt\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{{}}\n",
    "\n",
    "### Input:\n",
    "{{}}\n",
    "\n",
    "### Response:\n",
    "{{}}\"\"\"\n",
    "\n",
    "# G√©n√©ration\n",
    "prompt = alpaca_prompt.format(\n",
    "    \"Qu'est-ce que l'intelligence artificielle?\",\n",
    "    \"\",\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Le mod√®le est sp√©cialis√© pour le fran√ßais\n",
    "- Performances limit√©es pour les t√¢ches n√©cessitant un raisonnement complexe\n",
    "- Peut g√©n√©rer des informations incorrectes (hallucinations)\n",
    "\n",
    "## √âthique\n",
    "\n",
    "Ce mod√®le est destin√© √† des fins √©ducatives et de recherche. Les utilisateurs doivent √™tre conscients des biais potentiels et utiliser le mod√®le de mani√®re responsable.\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarde du README\n",
    "with open(model_save_path / \"README.md\", 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"üìù README cr√©√© pour Hugging Face Hub\")\n",
    "\n",
    "# Instructions pour la publication\n",
    "print(\"\\nüöÄ Instructions pour la publication sur Hugging Face Hub:\")\n",
    "print(\"1. Cr√©ez un compte sur Hugging Face: https://huggingface.co/join\")\n",
    "print(\"2. Installez l'CLI Hugging Face: pip install 'huggingface_hub[cli]'\")\n",
    "print(\"3. Connectez-vous: huggingface-cli login\")\n",
    "print(\"4. Cr√©ez un nouveau repository: huggingface-cli repo create Qwen3-4B-French-LoRA --type model\")\n",
    "print(\"5. Uploadez votre mod√®le: huggingface-cli upload ./models/lora_qwen3_french votre-username/Qwen3-4B-French-LoRA\")\n",
    "\n",
    "# Script de publication automatis√©\n",
    "publish_script = f\"\"\"#!/bin/bash\n",
    "# Script pour publier le mod√®le sur Hugging Face Hub\n",
    "\n",
    "# Remplacez VOTRE_USERNAME par votre username Hugging Face\n",
    "USERNAME=\"VOTRE_USERNAME\"\n",
    "MODEL_NAME=\"Qwen3-4B-French-LoRA\"\n",
    "MODEL_PATH=\"./models/lora_qwen3_french\"\n",
    "\n",
    "echo \"Publication du mod√®le $MODEL_NAME sur Hugging Face Hub...\"\n",
    "\n",
    "# V√©rification que le mod√®le existe\n",
    "if [ ! -d \"$MODEL_PATH\" ]; then\n",
    "    echo \"Erreur: Le mod√®le n'existe pas dans $MODEL_PATH\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Upload du mod√®le\n",
    "huggingface-cli upload $MODEL_PATH $USERNAME/$MODEL_NAME \\\n",
    "    --repo-type model \\\n",
    "    --private  # Changez √† --public pour un mod√®le public\n",
    "\n",
    "echo \"‚úÖ Mod√®le publi√© avec succ√®s!\"\n",
    "echo \"üîó Visitez: https://huggingface.co/$USERNAME/$MODEL_NAME\"\n",
    "\"\"\"\n",
    "\n",
    "with open(model_save_path / \"publish.sh\", 'w') as f:\n",
    "    f.write(publish_script)\n",
    "\n",
    "# Rendre le script ex√©cutable\n",
    "os.chmod(model_save_path / \"publish.sh\", 0o755)\n",
    "print(\"\\nüìú Script de publication cr√©√©: publish.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. R√©sum√© et conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© final et completion de TrackIO\n",
    "print(\"üéâ TP de fine-tuning LoRA avec Unsloth termin√©!\\n\")\n",
    "\n",
    "print(\"üìä R√©sum√© des r√©sultats:\")\n",
    "print(f\"   - Mod√®le de base: {model_name}\")\n",
    "print(f\"   - Param√®tres LoRA: rank={lora_config['r']}, alpha={lora_config['lora_alpha']}\")\n",
    "print(f\"   - Dataset d'entra√Ænement: {len(dataset_dict['train'])} exemples\")\n",
    "print(f\"   - √âpoques d'entra√Ænement: {training_args.num_train_epochs}\")\n",
    "print(f\"   - Perte finale: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"   - Perte validation: {eval_results.get('eval_loss', 'N/A')}\")\n",
    "print(f\"   - M√©moire GPU utilis√©e: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "print(f\"   - TrackIO: Activ√© et logg√©\")\n",
    "\n",
    "print(\"\\nüìÅ Fichiers cr√©√©s:\")\n",
    "print(f\"   - {model_save_path.name}/ (adaptateurs LoRA)\")\n",
    "print(f\"   - {model_save_path.name}/merged_16bit/ (mod√®le fusionn√© 16-bit)\")\n",
    "print(f\"   - {model_save_path.name}/merged_4bit/ (mod√®le fusionn√© 4-bit)\")\n",
    "print(f\"   - {model_save_path.name}/README.md (documentation)\")\n",
    "print(f\"   - {model_save_path.name}/publish.sh (script de publication)\")\n",
    "\n",
    "print(\"\\nüîç Points cl√©s appris:\")\n",
    "print(\"   1. LoRA permet de fine-tuner avec ~1% des param√®tres\")\n",
    "print(\"   2. QLoRA r√©duit significativement la m√©moire GPU\")\n",
    "print(\"   3. Unsloth acc√©l√®re l'entra√Ænement de 2-3x\")\n",
    "print(\"   4. TrackIO permet un suivi pr√©cis des exp√©riences\")\n",
    "print(\"   5. Plusieurs formats de sauvegarde pour diff√©rents cas d'usage\")\n",
    "\n",
    "print(\"\\nüöÄ Prochaines √©tapes:\")\n",
    "print(\"   1. Publier le mod√®le sur Hugging Face Hub\")\n",
    "print(\"   2. Tester avec des donn√©es r√©elles\")\n",
    "print(\"   3. Exp√©rimenter avec diff√©rents param√®tres LoRA\")\n",
    "print(\"   4. Essayer d'autres techniques de fine-tuning\")\n",
    "print(\"   5. Analyser les r√©sultats dans TrackIO dashboard\")\n",
    "\n",
    "# Finalisation de TrackIO\n",
    "trackio.log({\n",
    "    \"experiment_completed\": True,\n",
    "    \"completion_time\": time.time(),\n",
    "    \"model_saved\": True,\n",
    "    \"total_experiment_time\": time.time() - trackio._start_time if hasattr(trackio, '_start_time') else 0,\n",
    "    \"key_learnings\": [\n",
    "        \"Unsloth provides significant speedup for LoRA training\",\n",
    "        \"QLoRA enables training on consumer GPUs\",\n",
    "        \"Multiple save formats provide flexibility for deployment\",\n",
    "        \"TrackIO enables experiment tracking and comparison\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Completion de l'exp√©rience\n",
    "trackio.finish()\n",
    "\n",
    "print(\"\\nüìä TrackIO: Exp√©rience compl√©t√©e et sauvegard√©e!\")\n",
    "print(\"   - Visualisez les r√©sultats avec: trackio show\")\n",
    "print(\"   - Ou consultez le dashboard TrackIO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Pour aller plus loin\n",
    "\n",
    "### Exp√©rimentations sugg√©r√©es\n",
    "\n",
    "1. **Variation du rank LoRA**: Tester r=8, r=16, r=32 et comparer les performances\n",
    "2. **Diff√©rents taux d'apprentissage**: 1e-4, 2e-4, 5e-4\n",
    "3. **Augmentation du dataset**: G√©n√©rer plus de donn√©es synth√©tiques\n",
    "4. **Techniques avanc√©es**:\n",
    "   - DoRA (Weight-Decomposed LoRA)\n",
    "   - VeRA (Vector-based Random Matrix Adaptation)\n",
    "   - QLoRA avec diff√©rents niveaux de quantification\n",
    "\n",
    "### Optimisations possibles\n",
    "\n",
    "1. **Batch processing**: Augmenter la taille du batch si m√©moire permet\n",
    "2. **Gradient checkpointing**: Activ√© par d√©faut dans Unsloth\n",
    "3. **Mixed precision**: Utiliser bf16 si disponible\n",
    "4. **Data parallelism**: Pour multi-GPU\n",
    "\n",
    "### √âvaluation approfondie\n",
    "\n",
    "1. **Benchmarks automatiques**: Utiliser des benchmarks fran√ßais\n",
    "2. **√âvaluation humaine**: Qualit√© des r√©ponses g√©n√©r√©es\n",
    "3. **Comparaison baseline**: Contre le mod√®le de base\n",
    "4. **Analyse d'erreurs**: Comprendre les faiblesses du mod√®le\n",
    "\n",
    "### D√©ploiement\n",
    "\n",
    "1. **API REST**: Cr√©er une API pour le mod√®le\n",
    "2. **Optimisation inf√©rence**: vLLM, TensorRT-LLM\n",
    "3. **Quantification post-entra√Ænement**: GGUF, AWQ\n",
    "4. **Monitoring**: Suivi des performances en production\n",
    "\n",
    "### TrackIO avanc√©\n",
    "\n",
    "Pour analyser vos exp√©riences plus en d√©tail:\n",
    "\n",
    "```python\n",
    "# Comparer plusieurs exp√©riences\n",
    "trackio.compare_experiments([\n",
    "    \"qwen3-4b-lora-french-1\",\n",
    "    \"qwen3-4b-lora-french-2\",\n",
    "    \"qwen3-4b-lora-french-3\"\n",
    "])\n",
    "\n",
    "# Exporter les r√©sultats\n",
    "trackio.export_results(\"experiment_results.csv\")\n",
    "\n",
    "# Visualiser les courbes d'apprentissage\n",
    "trackio.plot_training_curves()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
