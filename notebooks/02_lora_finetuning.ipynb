{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Partie 2 : Fine-tuning de LLM avec LoRA\n",
    "\n",
    "## üìö Contexte\n",
    "\n",
    "Dans ce notebook, nous allons explorer le fine-tuning de mod√®les de langage avec la technique LoRA (Low-Rank Adaptation). Cette approche permet d'adapter des LLMs pour des t√¢ches sp√©cifiques.\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "1. **Comprendre** le fonctionnement de LoRA et ses avantages\n",
    "2. **Pr√©parer** le dataset synth√©tique g√©n√©r√© pr√©c√©demment\n",
    "3. **Configurer** un entra√Ænement LoRA avec Unsloth\n",
    "4. **Entra√Æner** un mod√®le Qwen3-4B sp√©cialis√© en fran√ßais\n",
    "5. **√âvaluer** les performances du mod√®le fine-tun√©\n",
    "6. **Publier** le mod√®le sur Hugging Face Hub\n",
    "\n",
    "## üîß Etapes\n",
    "\n",
    "Notre approche utilise :\n",
    "\n",
    "1. **Unsloth** : Biblioth√®que optimis√©e pour le fine-tuning LoRA\n",
    "2. **Qwen3-4B** : Mod√®le de base pr√©-entra√Æn√©\n",
    "3. **QLoRA** : Quantization + LoRA pour r√©duire la m√©moire GPU\n",
    "4. **Hugging Face Hub** : Publication et partage du mod√®le\n",
    "\n",
    "Resources √† garder sous la main durant le TP:\n",
    "\n",
    "- [LoRA Hyperparameters Guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide)\n",
    "- [What Model Should I Use?](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/what-model-should-i-use)\n",
    "- [Qwen3 instruct fine tune guide](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-Instruct.ipynb)"
   ],
   "id": "414f22cc011ad850"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T14:04:43.815441Z",
     "start_time": "2025-09-02T14:04:43.807337Z"
    }
   },
   "source": [
    "# Import des biblioth√®ques Unsloth et TrackIO\n",
    "import time\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import torch\n",
    "    import trackio\n",
    "    \n",
    "    # V√©rification GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device utilis√©: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erreur d'import: {e}\")\n",
    "    print(\"Veuillez ex√©cuter 'uv sync' pour installer les d√©pendances.\")"
   ],
   "id": "48d0567ba0f53445",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement et pr√©paration du dataset\n",
    "\n",
    "Nous chargeons le dataset synth√©tique g√©n√©r√© dans la partie pr√©c√©dente."
   ],
   "id": "d5a1cfa19fda637b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins\n",
    "data_dir = Path(\"../data/synthetic\")\n",
    "\n",
    "# Chargement du dataset\n",
    "print(\"Chargement du dataset synth√©tique...\")\n",
    "if (data_dir / \"synthetic_dataset_alpaca.json\").exists():\n",
    "    with open(data_dir / \"synthetic_dataset_alpaca.json\", 'r', encoding='utf-8') as f:\n",
    "        alpaca_data = json.load(f)\n",
    "    print(f\"‚úÖ Dataset charg√©: {len(alpaca_data)} paires instruction-r√©ponse\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset non trouv√©. Veuillez ex√©cuter le notebook 01 d'abord.\")\n",
    "    # Cr√©ation d'un dataset d'exemple\n",
    "    alpaca_data = [\n",
    "        {\n",
    "            \"instruction\": \"Qu'est-ce que l'apprentissage automatique?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"L'apprentissage automatique est un sous-domaine de l'intelligence artificielle qui permet aux syst√®mes d'apprendre √† partir de donn√©es sans √™tre explicitement programm√©s.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Conversion en DataFrame pour analyse\n",
    "df = pd.DataFrame(alpaca_data)\n",
    "print(f\"\\nüìä Statistiques du dataset:\")\n",
    "print(f\"   - Nombre total de paires: {len(df)}\")\n",
    "print(f\"   - Longueur moyenne des instructions: {df['instruction'].str.len().mean():.0f} caract√®res\")\n",
    "print(f\"   - Longueur moyenne des r√©ponses: {df['output'].str.len().mean():.0f} caract√®res\")\n",
    "\n",
    "# Affichage des premi√®res lignes\n",
    "print(\"\\nüìù Exemples de donn√©es:\")\n",
    "for i in range(min(3, len(alpaca_data))):\n",
    "    print(f\"\\n--- Exemple {i+1} ---\")\n",
    "    print(f\"Instruction: {alpaca_data[i]['instruction']}\")\n",
    "    print(f\"R√©ponse: {alpaca_data[i]['output'][:100]}...\")"
   ],
   "id": "f06aa19ae6eb026c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pr√©paration des donn√©es pour l'entra√Ænement\n",
    "\n",
    "Nous formatons les donn√©es selon le format attendu par Unsloth pour le fine-tuning instructionnel."
   ],
   "id": "7420809f708934e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template de formatage pour le mod√®le\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = \"<|im_end|>\"  # Token de fin pour Qwen3\n",
    "\n",
    "def format_prompts(examples):\n",
    "    \"\"\"\n",
    "    Formate les exemples selon le template Alpaca pour le fine-tuning.\n",
    "    \"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    \n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        # Formatage du prompt\n",
    "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Conversion en Dataset Hugging Face\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Application du formatage\n",
    "print(\"Formatage des donn√©es pour l'entra√Ænement...\")\n",
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "\n",
    "# Split train/validation\n",
    "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test']\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset pr√©par√©:\")\n",
    "print(f\"   - Training set: {len(dataset_dict['train'])} exemples\")\n",
    "print(f\"   - Validation set: {len(dataset_dict['validation'])} exemples\")\n",
    "\n",
    "# Affichage d'un exemple format√©\n",
    "print(\"\\nüìù Exemple format√©:\")\n",
    "print(dataset_dict['train'][0]['text'][:500] + \"...\")"
   ],
   "id": "80a378b29c798e14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Configuration du mod√®le et de LoRA avec TrackIO\n\nNous configurons le mod√®le Qwen3-4B avec les param√®tres LoRA optimis√©s selon les meilleures pratiques Unsloth, et initialisons TrackIO pour le suivi des exp√©riences.",
   "id": "a23be806e6959eb7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialisation de TrackIO pour le suivi des exp√©riences\nexperiment_name = f\"qwen3-4b-lora-french-{int(time.time())}\"\ntrackio.init(\n    project=\"llm-finetuning-tp\",\n    name=experiment_name,\n    config={\n        \"model\": \"Qwen/Qwen3-4B-Instruct-2507\",\n        \"dataset\": \"synthetic_french\",\n        \"framework\": \"unsloth\",\n        \"optimization\": \"qlora\"\n    }\n)\n\nprint(f\"üìä TrackIO initialis√© - Exp√©rience: {experiment_name}\")\n\n# Configuration du mod√®le avec param√®tres optimis√©s Unsloth\nmodel_name = \"Qwen/Qwen3-4B-Instruct-2507\"\nmax_seq_length = 2048  # Longueur maximale de s√©quence\ndtype = None  # D√©tection automatique du type\nload_in_4bit = True  # Utilisation de la quantification 4-bit\n\nprint(f\"Chargement du mod√®le {model_name}...\")\n\n# Chargement du mod√®le avec Unsloth\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n    token=None,  # Pas besoin de token pour les mod√®les publics\n)\n\nprint(\"‚úÖ Mod√®le charg√© avec succ√®s!\")\nprint(f\"   - M√©moire utilis√©e: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\nprint(f\"   - M√©moire r√©serv√©e: {torch.cuda.memory_reserved() / 1e9:.1f} GB\")\n\n# Log des informations syst√®me\ntrackio.log({\n    \"gpu_memory_allocated_gb\": torch.cuda.memory_allocated() / 1e9,\n    \"gpu_memory_reserved_gb\": torch.cuda.memory_reserved() / 1e9,\n    \"model_parameters_total\": sum(p.numel() for p in model.parameters()),\n    \"model_size_gb\": sum(p.numel() * p.element_size() for p in model.parameters()) / 1e9\n})",
   "id": "6090b9b0f1ec53e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration des param√®tres LoRA optimis√©s selon les meilleures pratiques Unsloth\nlora_config = {\n    \"r\": 16,  # Rank des matrices de mise √† jour\n    \"target_modules\": [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],  # Modules √† adapter\n    \"lora_alpha\": 32,  # Facteur d'√©chelle (typiquement 2*r)\n    \"lora_dropout\": 0.05,  # Dropout pour r√©gularisation\n    \"bias\": \"none\",  # Pas d'adaptation des biais\n    \"use_gradient_checkpointing\": \"unsloth\",  # Optimis√© Unsloth\n    \"random_state\": 3407,\n    \"use_rslora\": False,  # Ne pas utiliser scaled LoRA\n    \"loftq_config\": None,  # Configuration LoftQ (pour quantification avanc√©e)\n}\n\nprint(\"Configuration LoRA:\")\nfor key, value in lora_config.items():\n    print(f\"   - {key}: {value}\")\n\n# Application de la configuration LoRA\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    **lora_config\n)\n\nprint(\"\\\\n‚úÖ Configuration LoRA appliqu√©e!\")\n\n# Log des param√®tres LoRA\ntrackio.log({\n    \"lora_rank\": lora_config[\"r\"],\n    \"lora_alpha\": lora_config[\"lora_alpha\"],\n    \"lora_dropout\": lora_config[\"lora_dropout\"],\n    \"target_modules\": lora_config[\"target_modules\"],\n    \"gradient_checkpointing\": lora_config[\"use_gradient_checkpointing\"]\n})\n\n# Affichage des param√®tres entra√Ænables\ntrainable_stats = model.print_trainable_parameters()\nprint(f\"   - Param√®tres entra√Ænables: {trainable_stats}\")",
   "id": "c9f0e83fc0c01cde"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration de l'entra√Ænement\n",
    "\n",
    "Nous configurons les hyperparam√®tres d'entra√Ænement et le trainer."
   ],
   "id": "b583fa6ab7b8e60d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration des arguments d'entra√Ænement optimis√©s avec TrackIO\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_finetuned_model\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    num_train_epochs=3,  # Nombre d'√©poques\n    learning_rate=2e-4,  # Taux d'apprentissage optimal pour LoRA\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"./outputs\",\n    report_to=\"trackio\",  # Utilisation de TrackIO pour le tracking\n    save_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"loss\",\n    greater_is_better=False,\n    # Param√®tres suppl√©mentaires pour le monitoring\n    logging_first_step=True,\n    logging_dir=\"./logs\",\n    ddp_find_unused_parameters=False,\n)\n\nprint(\"Arguments d'entra√Ænement:\")\nprint(f\"   - Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"   - Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"   - Learning rate: {training_args.learning_rate}\")\nprint(f\"   - Epochs: {training_args.num_train_epochs}\")\nprint(f\"   - Warmup steps: {training_args.warmup_steps}\")\nprint(f\"   - Report to: {training_args.report_to}\")\n\n# Log des hyperparam√®tres\ntrackio.log({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"num_train_epochs\": training_args.num_train_epochs,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"optimizer\": training_args.optim,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"effective_batch_size\": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n})",
   "id": "b8ce796736c5f4d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"validation\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Ne pas packer les s√©quences\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configur√©!\")\n",
    "print(f\"   - √âchantillons d'entra√Ænement: {len(trainer.train_dataset)}\")\n",
    "print(f\"   - √âchantillons de validation: {len(trainer.eval_dataset)}\")"
   ],
   "id": "cfb73b038beaab69"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entra√Ænement du mod√®le\n",
    "\n",
    "**‚ö†Ô∏è Important** : Cette √©tape peut prendre du temps (15-30 minutes) selon votre GPU et la taille du dataset."
   ],
   "id": "d38f0d9c169089e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# D√©marrage de l'entra√Ænement avec monitoring TrackIO\nprint(\"üöÄ D√©marrage de l'entra√Ænement LoRA...\")\nprint(f\"   - Dataset: {len(dataset_dict['train'])} exemples\")\nprint(f\"   - Param√®tres LoRA: rank={lora_config['r']}, alpha={lora_config['lora_alpha']}\")\nprint(f\"   - Learning rate: {training_args.learning_rate}\")\nprint(f\"   - Epochs: {training_args.num_train_epochs}\")\nprint(f\"   - TrackIO: Activ√©\\\\n\")\n\n# Suivi de la m√©moire avant entra√Ænement\nif torch.cuda.is_available():\n    print(f\"M√©moire GPU avant entra√Ænement: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n\n# Fonction de monitoring GPU pour TrackIO\ndef log_gpu_stats():\n    if torch.cuda.is_available():\n        gpu_stats = {\n            \"gpu_memory_allocated_gb\": torch.cuda.memory_allocated() / 1e9,\n            \"gpu_memory_reserved_gb\": torch.cuda.memory_reserved() / 1e9,\n            \"gpu_utilization_percent\": torch.cuda.utilization(),\n        }\n        try:\n            # Tentative de lecture de la temp√©rature et de la puissance\n            gpu_stats.update({\n                \"gpu_temperature_c\": torch.cuda.temperature(),\n                \"gpu_power_watts\": torch.cuda.power_draw(),\n            })\n        except:\n            pass\n        trackio.log(gpu_stats)\n\n# Log initial\nlog_gpu_stats()\ntrackio.log({\n    \"training_start_time\": time.time(),\n    \"dataset_size\": len(dataset_dict['train']),\n    \"validation_size\": len(dataset_dict['validation'])\n})\n\n# Lancement de l'entra√Ænement\ntrainer_stats = trainer.train()\n\nprint(\"\\\\n‚úÖ Entra√Ænement termin√©!\")\nprint(f\"   - Perte finale: {trainer_stats.training_loss:.4f}\")\nprint(f\"   - Temps d'entra√Ænement: {trainer_stats.metrics.get('train_runtime', 0):.1f} secondes\")\n\n# Suivi de la m√©moire apr√®s entra√Ænement\nif torch.cuda.is_available():\n    print(f\"M√©moire GPU apr√®s entra√Ænement: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n\n# Log des r√©sultats finaux\nlog_gpu_stats()\ntrackio.log({\n    \"training_end_time\": time.time(),\n    \"final_training_loss\": trainer_stats.training_loss,\n    \"training_runtime_seconds\": trainer_stats.metrics.get('train_runtime', 0),\n    \"training_samples_per_second\": trainer_stats.metrics.get('train_samples_per_second', 0),\n    \"total_steps\": trainer.state.global_step\n})",
   "id": "521d0358aa3f6015"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. √âvaluation du mod√®le\n",
    "\n",
    "Nous √©valuons les performances du mod√®le fine-tun√© sur le jeu de validation."
   ],
   "id": "aa52f263c5a04946"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# √âvaluation sur le jeu de validation avec TrackIO\nprint(\"üìä √âvaluation du mod√®le sur le jeu de validation...\")\neval_results = trainer.evaluate()\n\nprint(\"\\\\nR√©sultats de l'√©valuation:\")\nfor key, value in eval_results.items():\n    if isinstance(value, (int, float)):\n        print(f\"   - {key}: {value:.4f}\")\n\n# Log des r√©sultats d'√©valuation\ntrackio.log({\n    \"eval_loss\": eval_results.get(\"eval_loss\", 0),\n    \"eval_runtime\": eval_results.get(\"eval_runtime\", 0),\n    \"eval_samples_per_second\": eval_results.get(\"eval_samples_per_second\", 0),\n    \"eval_steps_per_second\": eval_results.get(\"eval_steps_per_second\", 0)\n})\n\n# Visualisation de la courbe d'apprentissage\nif hasattr(trainer, \"state\") and trainer.state.log_history:\n    log_history = trainer.state.log_history\n    losses = [log.get('loss', 0) for log in log_history if 'loss' in log]\n    steps = [log.get('step', 0) for log in log_history if 'loss' in log]\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(steps, losses, 'b-', label='Training Loss')\n    plt.xlabel('Steps')\n    plt.ylabel('Loss')\n    plt.title('Courbe d\\\\'apprentissage')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    # Log des m√©triques de la courbe d'apprentissage\n    if losses:\n        trackio.log({\n            \"training_loss_min\": min(losses),\n            \"training_loss_final\": losses[-1],\n            \"training_loss_improvement\": losses[0] - losses[-1] if len(losses) > 1 else 0\n        })",
   "id": "9014dd50cfd5b551"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test d'inf√©rence\n",
    "\n",
    "Nous testons le mod√®le fine-tun√© sur quelques exemples pour √©valuer sa capacit√© √† g√©n√©rer des r√©ponses en fran√ßais."
   ],
   "id": "e4ee067c3ec8fec1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration du mod√®le pour l'inf√©rence optimis√©e\nFastLanguageModel.for_inference(model)\n\n# Fonction de g√©n√©ration optimis√©e\ndef generate_response(instruction: str, max_new_tokens: int = 512) -> str:\n    \"\"\"\n    G√©n√®re une r√©ponse √† partir d'une instruction avec param√®tres optimis√©s.\n    \"\"\"\n    prompt = alpaca_prompt.format(\n        instruction,\n        \"\",  # Input vide\n        \"\"  # Laisser vide pour la g√©n√©ration\n    )\n    \n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n    \n    # G√©n√©ration avec param√®tres optimis√©s pour le fran√ßais\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        use_cache=True,\n        temperature=0.7,\n        top_p=0.9,\n        top_k=50,\n        repetition_penalty=1.1,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n    \n    response = tokenizer.batch_decode(outputs)[0]\n    # Extraire seulement la r√©ponse g√©n√©r√©e\n    response = response.split(\"### Response:\")[1].strip()\n    return response\n\n# Test avec exemples vari√©s en fran√ßais\ntest_instructions = [\n    \"Qu'est-ce que l'intelligence artificielle?\",\n    \"Explique le principe du fine-tuning avec LoRA.\",\n    \"Quels sont les avantages de QLoRA par rapport au fine-tuning classique?\",\n    \"Comment fonctionne l'apprentissage par renforcement?\",\n    \"Qu'est-ce que la quantification 4-bit dans les LLMs?\"\n]\n\nprint(\"üß™ Tests d'inf√©rence:\\\\n\")\n\n# Log des tests d'inf√©rence\ninference_results = []\n\nfor i, instruction in enumerate(test_instructions):\n    print(f\"--- Test {i+1} ---\")\n    print(f\"Instruction: {instruction}\")\n    \n    start_time = time.time()\n    response = generate_response(instruction)\n    inference_time = time.time() - start_time\n    \n    print(f\"R√©ponse: {response[:300]}...\")\n    print(f\"Temps de g√©n√©ration: {inference_time:.2f} secondes\\\\n\")\n    \n    # Stocker les r√©sultats pour le logging\n    inference_results.append({\n        \"test_id\": i + 1,\n        \"instruction\": instruction,\n        \"response_length\": len(response),\n        \"inference_time\": inference_time,\n        \"tokens_per_second\": len(response.split()) / inference_time if inference_time > 0 else 0\n    })\n\n# Log des r√©sultats d'inf√©rence\ntrackio.log({\n    \"inference_tests\": inference_results,\n    \"avg_inference_time\": sum(r[\"inference_time\"] for r in inference_results) / len(inference_results),\n    \"avg_tokens_per_second\": sum(r[\"tokens_per_second\"] for r in inference_results) / len(inference_results)\n})",
   "id": "4dabf06bcd34bb29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyse des compromis LoRA\n",
    "\n",
    "Nous analysons les avantages et inconv√©nients de l'approche LoRA."
   ],
   "id": "196749d457db1581"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des param√®tres entra√Ænables\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(\"üìä Analyse des param√®tres:\")\n",
    "print(f\"   - Param√®tres totaux: {total_params:,}\")\n",
    "print(f\"   - Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "print(f\"   - Pourcentage entra√Ænable: {percentage:.2f}%\")\n",
    "\n",
    "# Comparaison des approches\n",
    "comparison_data = {\n",
    "    \"Approche\": [\"Full Fine-tuning\", \"LoRA (r=16)\", \"LoRA (r=8)\", \"LoRA (r=32)\"],\n",
    "    \"Param√®tres entra√Ænables\": [\n",
    "        f\"{total_params:,}\",\n",
    "        f\"{trainable_params:,}\",\n",
    "        f\"{trainable_params//2:,}\",\n",
    "        f\"{trainable_params*2:,}\"\n",
    "    ],\n",
    "    \"M√©moire GPU estim√©e\": [\"~24GB\", \"~8GB\", \"~6GB\", \"~10GB\"],\n",
    "    \"Temps d'entra√Ænement\": [\"100%\", \"~30%\", \"~20%\", \"~40%\"],\n",
    "    \"Performance relative\": [\"100%\", \"~95%\", \"~85%\", \"~98%\"]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüîÑ Comparaison des approches:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution des types de param√®tres\n",
    "param_types = ['Entra√Ænables', 'Gel√©s']\n",
    "param_counts = [trainable_params, total_params - trainable_params]\n",
    "\n",
    "axes[0].pie(param_counts, labels=param_types, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Distribution des param√®tres')\n",
    "\n",
    "# Efficacit√© m√©moire\n",
    "approaches = ['Full FT', 'LoRA r=16', 'LoRA r=8', 'LoRA r=32']\n",
    "memory_usage = [100, 33, 25, 42]  # Pourcentage du full fine-tuning\n",
    "\n",
    "axes[1].bar(approaches, memory_usage, color=['red', 'green', 'blue', 'orange'])\n",
    "axes[1].set_ylabel('M√©moire GPU relative (%)')\n",
    "axes[1].set_title('Efficacit√© m√©moire')\n",
    "axes[1].set_ylim(0, 120)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for i, v in enumerate(memory_usage):\n",
    "    axes[1].text(i, v + 2, f'{v}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "51d6aea9839e2f18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sauvegarde du mod√®le\n",
    "\n",
    "Nous sauvegardons le mod√®le fine-tun√© pour une utilisation future."
   ],
   "id": "c830c3b187c84f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du dossier de sauvegarde\n",
    "model_save_path = Path(\"../models/lora_qwen3_french\")\n",
    "model_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Sauvegarde du mod√®le dans {model_save_path}...\")\n",
    "\n",
    "# Sauvegarde du mod√®le LoRA\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Sauvegarde de la configuration\n",
    "config = {\n",
    "    \"base_model\": model_name,\n",
    "    \"lora_config\": lora_config,\n",
    "    \"training_args\": {\n",
    "        \"num_train_epochs\": training_args.num_train_epochs,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"train_size\": len(dataset_dict['train']),\n",
    "        \"val_size\": len(dataset_dict['validation']),\n",
    "        \"source\": \"synthetic_dataset\"\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"final_loss\": trainer_stats.training_loss,\n",
    "        \"eval_results\": eval_results\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(model_save_path / \"config.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le sauvegard√© avec succ√®s!\")\n",
    "print(f\"   - Poids du mod√®le: {sum(os.path.getsize(model_save_path/f) for f in os.listdir(model_save_path) if os.path.isfile(model_save_path/f)) / 1e6:.1f} MB\")\n",
    "\n",
    "# Optionnel: Sauvegarde en 16-bit pour l'inf√©rence\n",
    "print(\"\\nüíæ Sauvegarde en 16-bit pour l'inf√©rence...\")\n",
    "model.save_pretrained_merged(\n",
    "    model_save_path / \"merged_16bit\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\"\n",
    ")\n",
    "print(\"‚úÖ Sauvegarde 16-bit termin√©e!\")"
   ],
   "id": "8468fb1f8a823fe0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Pr√©paration pour la publication sur Hugging Face\n",
    "\n",
    "Nous pr√©parons le mod√®le pour sa publication sur Hugging Face Hub."
   ],
   "id": "888dc61210c4a1f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du README pour Hugging Face\n",
    "readme_content = f\"\"\"---\n",
    "library_name: transformers\n",
    "license: mit\n",
    "base_model: {model_name}\n",
    "tags:\n",
    "- trl\n",
    "- sft\n",
    "- generated_from_trainer\n",
    "- french\n",
    "- lora\n",
    "- qwen3\n",
    "model-index:\n",
    "- name: Qwen3-4B-French-LoRA\n",
    "  results: []\n",
    "---\n",
    "\n",
    "# Qwen3-4B-French-LoRA\n",
    "\n",
    "## Description\n",
    "\n",
    "Ce mod√®le est une version fine-tun√©e de Qwen3-4B-Instruct sp√©cialis√©e pour le fran√ßais. Il a √©t√© entra√Æn√© sur un dataset synth√©tique d'instructions-r√©ponses en fran√ßais.\n",
    "\n",
    "## D√©tails d'entra√Ænement\n",
    "\n",
    "- **Base Model**: {model_name}\n",
    "- **Training Framework**: Unsloth + TRL\n",
    "- **Quantization**: 4-bit (QLoRA)\n",
    "- **LoRA Rank**: {lora_config['r']}\n",
    "- **LoRA Alpha**: {lora_config['lora_alpha']}\n",
    "- **Training Epochs**: {training_args.num_train_epochs}\n",
    "- **Learning Rate**: {training_args.learning_rate}\n",
    "- **Dataset Size**: {len(dataset_dict['train'])} examples\n",
    "\n",
    "## Performance\n",
    "\n",
    "- **Training Loss**: {trainer_stats.training_loss:.4f}\n",
    "- **Validation Loss**: {eval_results.get('eval_loss', 'N/A')}\n",
    "\n",
    "## Utilisation\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Chargement du mod√®le\n",
    "model_path = \"votre-username/Qwen3-4B-French-LoRA\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Formatage du prompt\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{{}}\n",
    "\n",
    "### Input:\n",
    "{{}}\n",
    "\n",
    "### Response:\n",
    "{{}}\"\"\"\n",
    "\n",
    "# G√©n√©ration\n",
    "prompt = alpaca_prompt.format(\n",
    "    \"Qu'est-ce que l'intelligence artificielle?\",\n",
    "    \"\",\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Le mod√®le est sp√©cialis√© pour le fran√ßais\n",
    "- Performances limit√©es pour les t√¢ches n√©cessitant un raisonnement complexe\n",
    "- Peut g√©n√©rer des informations incorrectes (hallucinations)\n",
    "\n",
    "## √âthique\n",
    "\n",
    "Ce mod√®le est destin√© √† des fins √©ducatives et de recherche. Les utilisateurs doivent √™tre conscients des biais potentiels et utiliser le mod√®le de mani√®re responsable.\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarde du README\n",
    "with open(model_save_path / \"README.md\", 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"üìù README cr√©√© pour Hugging Face Hub\")\n",
    "\n",
    "# Instructions pour la publication\n",
    "print(\"\\nüöÄ Instructions pour la publication sur Hugging Face Hub:\")\n",
    "print(\"1. Cr√©ez un compte sur Hugging Face: https://huggingface.co/join\")\n",
    "print(\"2. Installez l'CLI Hugging Face: pip install 'huggingface_hub[cli]'\")\n",
    "print(\"3. Connectez-vous: huggingface-cli login\")\n",
    "print(\"4. Cr√©ez un nouveau repository: huggingface-cli repo create Qwen3-4B-French-LoRA --type model\")\n",
    "print(\"5. Uploadez votre mod√®le: huggingface-cli upload ./models/lora_qwen3_french votre-username/Qwen3-4B-French-LoRA\")\n",
    "\n",
    "# Script de publication automatis√©\n",
    "publish_script = f\"\"\"#!/bin/bash\n",
    "# Script pour publier le mod√®le sur Hugging Face Hub\n",
    "\n",
    "# Remplacez VOTRE_USERNAME par votre username Hugging Face\n",
    "USERNAME=\"VOTRE_USERNAME\"\n",
    "MODEL_NAME=\"Qwen3-4B-French-LoRA\"\n",
    "MODEL_PATH=\"./models/lora_qwen3_french\"\n",
    "\n",
    "echo \"Publication du mod√®le $MODEL_NAME sur Hugging Face Hub...\"\n",
    "\n",
    "# V√©rification que le mod√®le existe\n",
    "if [ ! -d \"$MODEL_PATH\" ]; then\n",
    "    echo \"Erreur: Le mod√®le n'existe pas dans $MODEL_PATH\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Upload du mod√®le\n",
    "huggingface-cli upload $MODEL_PATH $USERNAME/$MODEL_NAME \\\n",
    "    --repo-type model \\\n",
    "    --private  # Changez √† --public pour un mod√®le public\n",
    "\n",
    "echo \"‚úÖ Mod√®le publi√© avec succ√®s!\"\n",
    "echo \"üîó Visitez: https://huggingface.co/$USERNAME/$MODEL_NAME\"\n",
    "\"\"\"\n",
    "\n",
    "with open(model_save_path / \"publish.sh\", 'w') as f:\n",
    "    f.write(publish_script)\n",
    "\n",
    "# Rendre le script ex√©cutable\n",
    "os.chmod(model_save_path / \"publish.sh\", 0o755)\n",
    "print(\"\\nüìú Script de publication cr√©√©: publish.sh\")"
   ],
   "id": "9f233d03f8940db3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. R√©sum√© et conclusions"
   ],
   "id": "c09dc8c799895d5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# R√©sum√© final et completion de TrackIO\nprint(\"üéâ TP de fine-tuning LoRA termin√©!\\\\n\")\n\nprint(\"üìä R√©sum√© des r√©sultats:\")\nprint(f\"   - Mod√®le de base: {model_name}\")\nprint(f\"   - Param√®tres LoRA: rank={lora_config['r']}, alpha={lora_config['lora_alpha']}\")\nprint(f\"   - Dataset d'entra√Ænement: {len(dataset_dict['train'])} exemples\")\nprint(f\"   - √âpoques d'entra√Ænement: {training_args.num_train_epochs}\")\nprint(f\"   - Perte finale: {trainer_stats.training_loss:.4f}\")\nprint(f\"   - Perte validation: {eval_results.get('eval_loss', 'N/A')}\")\nprint(f\"   - M√©moire GPU utilis√©e: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\nprint(f\"   - TrackIO: Activ√© et logg√©\")\n\nprint(\"\\\\nüìÅ Fichiers cr√©√©s:\")\nprint(f\"   - {model_save_path.name}/ (mod√®le LoRA)\")\nprint(f\"   - {model_save_path.name}/merged_16bit/ (mod√®le fusionn√©)\")\nprint(f\"   - {model_save_path.name}/README.md (documentation)\")\nprint(f\"   - {model_save_path.name}/publish.sh (script de publication)\")\n\nprint(\"\\\\nüîç Points cl√©s appris:\")\nprint(\"   1. LoRA permet de fine-tuner avec ~1% des param√®tres\")\nprint(\"   2. QLoRA r√©duit significativement la m√©moire GPU\")\nprint(\"   3. Unsloth acc√©l√®re l'entra√Ænement de 2-3x\")\nprint(\"   4. TrackIO permet un suivi pr√©cis des exp√©riences\")\nprint(\"   5. Le mod√®le reste petit et facile √† partager\")\n\nprint(\"\\\\nüöÄ Prochaines √©tapes:\")\nprint(\"   1. Publier le mod√®le sur Hugging Face Hub\")\nprint(\"   2. Tester avec des donn√©es r√©elles\")\nprint(\"   3. Exp√©rimenter avec diff√©rents param√®tres LoRA\")\nprint(\"   4. Essayer d'autres techniques de fine-tuning\")\nprint(\"   5. Analyser les r√©sultats dans TrackIO dashboard\")\n\n# Finalisation de TrackIO\ntrackio.log({\n    \"experiment_completed\": True,\n    \"completion_time\": time.time(),\n    \"model_saved\": True,\n    \"total_experiment_time\": time.time() - trackio._start_time if hasattr(trackio, '_start_time') else 0\n})\n\n# Completion de l'exp√©rience\ntrackio.finish()\n\nprint(\"\\\\nüìä TrackIO: Exp√©rience compl√©t√©e et sauvegard√©e!\")\nprint(\"   - Visualisez les r√©sultats avec: trackio show\")\nprint(\"   - Ou consultez le dashboard TrackIO\")",
   "id": "8f91bcfbb09fa122"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 15. Pour aller plus loin\n\n### Exp√©rimentations sugg√©r√©es\n\n1. **Variation du rank LoRA**: Tester r=8, r=16, r=32 et comparer les performances\n2. **Diff√©rents taux d'apprentissage**: 1e-4, 2e-4, 5e-4\n3. **Augmentation du dataset**: G√©n√©rer plus de donn√©es synth√©tiques\n4. **Techniques avanc√©es**:\n   - DoRA (Weight-Decomposed LoRA)\n   - VeRA (Vector-based Random Matrix Adaptation)\n   - QLoRA avec diff√©rents niveaux de quantification\n\n### Optimisations possibles\n\n1. **Batch processing**: Augmenter la taille du batch si m√©moire permet\n2. **Gradient checkpointing**: Activ√© par d√©faut dans Unsloth\n3. **Mixed precision**: Utiliser bf16 si disponible\n4. **Data parallelism**: Pour multi-GPU\n\n### √âvaluation approfondie\n\n1. **Benchmarks automatiques**: Utiliser des benchmarks fran√ßais\n2. **√âvaluation humaine**: Qualit√© des r√©ponses g√©n√©r√©es\n3. **Comparaison baseline**: Contre le mod√®le de base\n4. **Analyse d'erreurs**: Comprendre les faiblesses du mod√®le\n\n### D√©ploiement\n\n1. **API REST**: Cr√©er une API pour le mod√®le\n2. **Optimisation inf√©rence**: vLLM, TensorRT-LLM\n3. **Quantification post-entra√Ænement**: GGUF, AWQ\n4. **Monitoring**: Suivi des performances en production\n\n### TrackIO avanc√©\n\nPour analyser vos exp√©riences plus en d√©tail:\n\n```python\n# Comparer plusieurs exp√©riences\ntrackio.compare_experiments([\n    \"qwen3-4b-lora-french-1\",\n    \"qwen3-4b-lora-french-2\",\n    \"qwen3-4b-lora-french-3\"\n])\n\n# Exporter les r√©sultats\ntrackio.export_results(\"experiment_results.csv\")\n\n# Visualiser les courbes d'apprentissage\ntrackio.plot_training_curves()\n```",
   "id": "c9bf101e7d4fbd00"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
