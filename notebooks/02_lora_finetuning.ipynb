{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Partie 2 : Fine-tuning de LLM avec LoRA\n",
    "\n",
    "## üìö Contexte\n",
    "\n",
    "Dans ce notebook, nous allons explorer le fine-tuning de mod√®les de langage avec la technique LoRA (Low-Rank Adaptation). Cette approche permet d'adapter des LLMs pour des t√¢ches sp√©cifiques.\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "1. **Comprendre** le fonctionnement de LoRA et ses avantages\n",
    "2. **Pr√©parer** le dataset synth√©tique g√©n√©r√© pr√©c√©demment\n",
    "3. **Configurer** un entra√Ænement LoRA avec Unsloth\n",
    "4. **Entra√Æner** un mod√®le Qwen3-4B sp√©cialis√© en fran√ßais\n",
    "5. **√âvaluer** les performances du mod√®le fine-tun√©\n",
    "6. **Publier** le mod√®le sur Hugging Face Hub\n",
    "\n",
    "## üîß Architecture\n",
    "\n",
    "Notre approche utilise :\n",
    "\n",
    "1. **Unsloth** : Biblioth√®que optimis√©e pour le fine-tuning LoRA\n",
    "2. **Qwen3-4B** : Mod√®le de base pr√©-entra√Æn√©\n",
    "3. **QLoRA** : Quantization + LoRA pour r√©duire la m√©moire GPU\n",
    "4. **Hugging Face Hub** : Publication et partage du mod√®le"
   ],
   "id": "414f22cc011ad850"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Import des biblioth√®ques Unsloth\n\n**Note importante** : Toutes les d√©pendances sont g√©r√©es via `uv sync` depuis le pyproject.toml.\n\n### Installation pr√©alable :\n```bash\n# Installation standard\nuv sync\n\n# Pour GPU CUDA support\nuv sync --extra gpu\n\n# Pour le d√©veloppement\nuv sync --extra dev\n```\n\nSi vous rencontrez des erreurs d'importation, v√©rifiez que vous avez bien ex√©cut√© la commande d'installation.",
   "id": "95415a02520743b8"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T13:15:20.853875Z",
     "start_time": "2025-09-02T13:15:20.751232Z"
    }
   },
   "source": "# Import des biblioth√®ques Unsloth\ntry:\n    from unsloth import FastLanguageModel\n    print(\"‚úÖ Unsloth import√© avec succ√®s!\")\n    \n    # V√©rification GPU\n    import torch\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device utilis√©: {device}\")\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name()}\")\n        print(f\"M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n        \nexcept ImportError as e:\n    print(f\"‚ùå Erreur d'import: {e}\")\n    print(\"Veuillez ex√©cuter 'uv sync' pour installer les d√©pendances.\")\n    print(\"Pour GPU support: uv sync --extra gpu\")",
   "id": "48d0567ba0f53445",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation d'Unsloth\n",
    "\n",
    "**Note importante** : Unsloth est une biblioth√®que optimis√©e qui acc√©l√®re significativement le fine-tuning LoRA."
   ],
   "id": "2116af5a252b1e3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation d'Unsloth (n√©cessite red√©marrage du kernel)\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes\n",
    "\n",
    "# Import apr√®s installation\n",
    "from unsloth import FastLanguageModel"
   ],
   "id": "fa9d4ecb8fc6eccf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement et pr√©paration du dataset\n",
    "\n",
    "Nous chargeons le dataset synth√©tique g√©n√©r√© dans la partie pr√©c√©dente."
   ],
   "id": "d5a1cfa19fda637b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins\n",
    "data_dir = Path(\"../data/synthetic\")\n",
    "\n",
    "# Chargement du dataset\n",
    "print(\"Chargement du dataset synth√©tique...\")\n",
    "if (data_dir / \"synthetic_dataset_alpaca.json\").exists():\n",
    "    with open(data_dir / \"synthetic_dataset_alpaca.json\", 'r', encoding='utf-8') as f:\n",
    "        alpaca_data = json.load(f)\n",
    "    print(f\"‚úÖ Dataset charg√©: {len(alpaca_data)} paires instruction-r√©ponse\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset non trouv√©. Veuillez ex√©cuter le notebook 01 d'abord.\")\n",
    "    # Cr√©ation d'un dataset d'exemple\n",
    "    alpaca_data = [\n",
    "        {\n",
    "            \"instruction\": \"Qu'est-ce que l'apprentissage automatique?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"L'apprentissage automatique est un sous-domaine de l'intelligence artificielle qui permet aux syst√®mes d'apprendre √† partir de donn√©es sans √™tre explicitement programm√©s.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Conversion en DataFrame pour analyse\n",
    "df = pd.DataFrame(alpaca_data)\n",
    "print(f\"\\nüìä Statistiques du dataset:\")\n",
    "print(f\"   - Nombre total de paires: {len(df)}\")\n",
    "print(f\"   - Longueur moyenne des instructions: {df['instruction'].str.len().mean():.0f} caract√®res\")\n",
    "print(f\"   - Longueur moyenne des r√©ponses: {df['output'].str.len().mean():.0f} caract√®res\")\n",
    "\n",
    "# Affichage des premi√®res lignes\n",
    "print(\"\\nüìù Exemples de donn√©es:\")\n",
    "for i in range(min(3, len(alpaca_data))):\n",
    "    print(f\"\\n--- Exemple {i+1} ---\")\n",
    "    print(f\"Instruction: {alpaca_data[i]['instruction']}\")\n",
    "    print(f\"R√©ponse: {alpaca_data[i]['output'][:100]}...\")"
   ],
   "id": "f06aa19ae6eb026c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pr√©paration des donn√©es pour l'entra√Ænement\n",
    "\n",
    "Nous formatons les donn√©es selon le format attendu par Unsloth pour le fine-tuning instructionnel."
   ],
   "id": "7420809f708934e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template de formatage pour le mod√®le\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = \"<|im_end|>\"  # Token de fin pour Qwen3\n",
    "\n",
    "def format_prompts(examples):\n",
    "    \"\"\"\n",
    "    Formate les exemples selon le template Alpaca pour le fine-tuning.\n",
    "    \"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    \n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        # Formatage du prompt\n",
    "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Conversion en Dataset Hugging Face\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Application du formatage\n",
    "print(\"Formatage des donn√©es pour l'entra√Ænement...\")\n",
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "\n",
    "# Split train/validation\n",
    "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test']\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset pr√©par√©:\")\n",
    "print(f\"   - Training set: {len(dataset_dict['train'])} exemples\")\n",
    "print(f\"   - Validation set: {len(dataset_dict['validation'])} exemples\")\n",
    "\n",
    "# Affichage d'un exemple format√©\n",
    "print(\"\\nüìù Exemple format√©:\")\n",
    "print(dataset_dict['train'][0]['text'][:500] + \"...\")"
   ],
   "id": "80a378b29c798e14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration du mod√®le et de LoRA\n",
    "\n",
    "Nous configurons le mod√®le Qwen3-4B avec les param√®tres LoRA pour un fine-tuning efficace."
   ],
   "id": "a23be806e6959eb7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du mod√®le\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "max_seq_length = 2048  # Longueur maximale de s√©quence\n",
    "dtype = None  # D√©tection automatique du type\n",
    "load_in_4bit = True  # Utilisation de la quantification 4-bit\n",
    "\n",
    "print(f\"Chargement du mod√®le {model_name}...\")\n",
    "\n",
    "# Chargement du mod√®le avec Unsloth\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© avec succ√®s!\")\n",
    "print(f\"   - M√©moire utilis√©e: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "print(f\"   - M√©moire r√©serv√©e: {torch.cuda.memory_reserved() / 1e9:.1f} GB\")"
   ],
   "id": "6090b9b0f1ec53e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des param√®tres LoRA\n",
    "lora_config = {\n",
    "    \"r\": 16,  # Rank des matrices de mise √† jour\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],  # Modules √† adapter\n",
    "    \"lora_alpha\": 32,  # Facteur d'√©chelle\n",
    "    \"lora_dropout\": 0.05,  # Dropout pour r√©gularisation\n",
    "    \"bias\": \"none\",  # Pas d'adaptation des biais\n",
    "    \"use_gradient_checkpointing\": True,  # Pour √©conomiser la m√©moire\n",
    "    \"random_state\": 3407,\n",
    "    \"use_rslora\": False,  # Ne pas utiliser scaled LoRA\n",
    "    \"loftq_config\": None,  # Configuration LoftQ (pour quantification avanc√©e)\n",
    "}\n",
    "\n",
    "print(\"Configuration LoRA:\")\n",
    "for key, value in lora_config.items():\n",
    "    print(f\"   - {key}: {value}\")\n",
    "\n",
    "# Application de la configuration LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    **lora_config\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Configuration LoRA appliqu√©e!\")\n",
    "print(f\"   - Param√®tres entra√Ænables: {model.print_trainable_parameters()}\")"
   ],
   "id": "c9f0e83fc0c01cde"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration de l'entra√Ænement\n",
    "\n",
    "Nous configurons les hyperparam√®tres d'entra√Ænement et le trainer."
   ],
   "id": "b583fa6ab7b8e60d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des arguments d'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetuned_model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    num_train_epochs=3,  # Nombre d'√©poques\n",
    "    learning_rate=2e-4,  # Taux d'apprentissage\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"./outputs\",\n",
    "    report_to=\"none\",  # D√©sactiver wandb/tensorboard\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "print(\"Arguments d'entra√Ænement:\")\n",
    "print(f\"   - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   - Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   - Warmup steps: {training_args.warmup_steps}\")"
   ],
   "id": "b8ce796736c5f4d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"validation\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Ne pas packer les s√©quences\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configur√©!\")\n",
    "print(f\"   - √âchantillons d'entra√Ænement: {len(trainer.train_dataset)}\")\n",
    "print(f\"   - √âchantillons de validation: {len(trainer.eval_dataset)}\")"
   ],
   "id": "cfb73b038beaab69"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entra√Ænement du mod√®le\n",
    "\n",
    "**‚ö†Ô∏è Important** : Cette √©tape peut prendre du temps (15-30 minutes) selon votre GPU et la taille du dataset."
   ],
   "id": "d38f0d9c169089e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©marrage de l'entra√Ænement\n",
    "print(\"üöÄ D√©marrage de l'entra√Ænement LoRA...\")\n",
    "print(f\"   - Dataset: {len(dataset_dict['train'])} exemples\")\n",
    "print(f\"   - Param√®tres LoRA: rank={lora_config['r']}, alpha={lora_config['lora_alpha']}\")\n",
    "print(f\"   - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   - Epochs: {training_args.num_train_epochs}\\n\")\n",
    "\n",
    "# Suivi de la m√©moire avant entra√Ænement\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"M√©moire GPU avant entra√Ænement: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "\n",
    "# Lancement de l'entra√Ænement\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√©!\")\n",
    "print(f\"   - Perte finale: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"   - Temps d'entra√Ænement: {trainer_stats.metrics.get('train_runtime', 0):.1f} secondes\")\n",
    "\n",
    "# Suivi de la m√©moire apr√®s entra√Ænement\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"M√©moire GPU apr√®s entra√Ænement: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")"
   ],
   "id": "521d0358aa3f6015"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. √âvaluation du mod√®le\n",
    "\n",
    "Nous √©valuons les performances du mod√®le fine-tun√© sur le jeu de validation."
   ],
   "id": "aa52f263c5a04946"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation sur le jeu de validation\n",
    "print(\"üìä √âvaluation du mod√®le sur le jeu de validation...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nR√©sultats de l'√©valuation:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   - {key}: {value:.4f}\")\n",
    "\n",
    "# Visualisation de la courbe d'apprentissage\n",
    "if hasattr(trainer, \"state\") and trainer.state.log_history:\n",
    "    log_history = trainer.state.log_history\n",
    "    losses = [log.get('loss', 0) for log in log_history if 'loss' in log]\n",
    "    steps = [log.get('step', 0) for log in log_history if 'loss' in log]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(steps, losses, 'b-', label='Training Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Courbe d\\'apprentissage')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ],
   "id": "9014dd50cfd5b551"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test d'inf√©rence\n",
    "\n",
    "Nous testons le mod√®le fine-tun√© sur quelques exemples pour √©valuer sa capacit√© √† g√©n√©rer des r√©ponses en fran√ßais."
   ],
   "id": "e4ee067c3ec8fec1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du mod√®le pour l'inf√©rence\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Fonction de g√©n√©ration\n",
    "def generate_response(instruction: str, max_new_tokens: int = 512) -> str:\n",
    "    \"\"\"\n",
    "    G√©n√®re une r√©ponse √† partir d'une instruction.\n",
    "    \"\"\"\n",
    "    prompt = alpaca_prompt.format(\n",
    "        instruction,\n",
    "        \"\",  # Input vide\n",
    "        \"\"  # Laisser vide pour la g√©n√©ration\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    # Extraire seulement la r√©ponse g√©n√©r√©e\n",
    "    response = response.split(\"### Response:\")[1].strip()\n",
    "    return response\n",
    "\n",
    "# Test avec quelques exemples\n",
    "test_instructions = [\n",
    "    \"Qu'est-ce que l'intelligence artificielle?\",\n",
    "    \"Explique le principe du fine-tuning.\",\n",
    "    \"Quels sont les avantages de LoRA?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Tests d'inf√©rence:\\n\")\n",
    "for i, instruction in enumerate(test_instructions):\n",
    "    print(f\"--- Test {i+1} ---\")\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    response = generate_response(instruction)\n",
    "    print(f\"R√©ponse: {response[:300]}...\\n\")"
   ],
   "id": "4dabf06bcd34bb29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyse des compromis LoRA\n",
    "\n",
    "Nous analysons les avantages et inconv√©nients de l'approche LoRA."
   ],
   "id": "196749d457db1581"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des param√®tres entra√Ænables\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(\"üìä Analyse des param√®tres:\")\n",
    "print(f\"   - Param√®tres totaux: {total_params:,}\")\n",
    "print(f\"   - Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "print(f\"   - Pourcentage entra√Ænable: {percentage:.2f}%\")\n",
    "\n",
    "# Comparaison des approches\n",
    "comparison_data = {\n",
    "    \"Approche\": [\"Full Fine-tuning\", \"LoRA (r=16)\", \"LoRA (r=8)\", \"LoRA (r=32)\"],\n",
    "    \"Param√®tres entra√Ænables\": [\n",
    "        f\"{total_params:,}\",\n",
    "        f\"{trainable_params:,}\",\n",
    "        f\"{trainable_params//2:,}\",\n",
    "        f\"{trainable_params*2:,}\"\n",
    "    ],\n",
    "    \"M√©moire GPU estim√©e\": [\"~24GB\", \"~8GB\", \"~6GB\", \"~10GB\"],\n",
    "    \"Temps d'entra√Ænement\": [\"100%\", \"~30%\", \"~20%\", \"~40%\"],\n",
    "    \"Performance relative\": [\"100%\", \"~95%\", \"~85%\", \"~98%\"]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüîÑ Comparaison des approches:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution des types de param√®tres\n",
    "param_types = ['Entra√Ænables', 'Gel√©s']\n",
    "param_counts = [trainable_params, total_params - trainable_params]\n",
    "\n",
    "axes[0].pie(param_counts, labels=param_types, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Distribution des param√®tres')\n",
    "\n",
    "# Efficacit√© m√©moire\n",
    "approaches = ['Full FT', 'LoRA r=16', 'LoRA r=8', 'LoRA r=32']\n",
    "memory_usage = [100, 33, 25, 42]  # Pourcentage du full fine-tuning\n",
    "\n",
    "axes[1].bar(approaches, memory_usage, color=['red', 'green', 'blue', 'orange'])\n",
    "axes[1].set_ylabel('M√©moire GPU relative (%)')\n",
    "axes[1].set_title('Efficacit√© m√©moire')\n",
    "axes[1].set_ylim(0, 120)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for i, v in enumerate(memory_usage):\n",
    "    axes[1].text(i, v + 2, f'{v}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "51d6aea9839e2f18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sauvegarde du mod√®le\n",
    "\n",
    "Nous sauvegardons le mod√®le fine-tun√© pour une utilisation future."
   ],
   "id": "c830c3b187c84f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du dossier de sauvegarde\n",
    "model_save_path = Path(\"../models/lora_qwen3_french\")\n",
    "model_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Sauvegarde du mod√®le dans {model_save_path}...\")\n",
    "\n",
    "# Sauvegarde du mod√®le LoRA\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Sauvegarde de la configuration\n",
    "config = {\n",
    "    \"base_model\": model_name,\n",
    "    \"lora_config\": lora_config,\n",
    "    \"training_args\": {\n",
    "        \"num_train_epochs\": training_args.num_train_epochs,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"train_size\": len(dataset_dict['train']),\n",
    "        \"val_size\": len(dataset_dict['validation']),\n",
    "        \"source\": \"synthetic_dataset\"\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"final_loss\": trainer_stats.training_loss,\n",
    "        \"eval_results\": eval_results\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(model_save_path / \"config.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le sauvegard√© avec succ√®s!\")\n",
    "print(f\"   - Poids du mod√®le: {sum(os.path.getsize(model_save_path/f) for f in os.listdir(model_save_path) if os.path.isfile(model_save_path/f)) / 1e6:.1f} MB\")\n",
    "\n",
    "# Optionnel: Sauvegarde en 16-bit pour l'inf√©rence\n",
    "print(\"\\nüíæ Sauvegarde en 16-bit pour l'inf√©rence...\")\n",
    "model.save_pretrained_merged(\n",
    "    model_save_path / \"merged_16bit\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\"\n",
    ")\n",
    "print(\"‚úÖ Sauvegarde 16-bit termin√©e!\")"
   ],
   "id": "8468fb1f8a823fe0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Pr√©paration pour la publication sur Hugging Face\n",
    "\n",
    "Nous pr√©parons le mod√®le pour sa publication sur Hugging Face Hub."
   ],
   "id": "888dc61210c4a1f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du README pour Hugging Face\n",
    "readme_content = f\"\"\"---\n",
    "library_name: transformers\n",
    "license: mit\n",
    "base_model: {model_name}\n",
    "tags:\n",
    "- trl\n",
    "- sft\n",
    "- generated_from_trainer\n",
    "- french\n",
    "- lora\n",
    "- qwen3\n",
    "model-index:\n",
    "- name: Qwen3-4B-French-LoRA\n",
    "  results: []\n",
    "---\n",
    "\n",
    "# Qwen3-4B-French-LoRA\n",
    "\n",
    "## Description\n",
    "\n",
    "Ce mod√®le est une version fine-tun√©e de Qwen3-4B-Instruct sp√©cialis√©e pour le fran√ßais. Il a √©t√© entra√Æn√© sur un dataset synth√©tique d'instructions-r√©ponses en fran√ßais.\n",
    "\n",
    "## D√©tails d'entra√Ænement\n",
    "\n",
    "- **Base Model**: {model_name}\n",
    "- **Training Framework**: Unsloth + TRL\n",
    "- **Quantization**: 4-bit (QLoRA)\n",
    "- **LoRA Rank**: {lora_config['r']}\n",
    "- **LoRA Alpha**: {lora_config['lora_alpha']}\n",
    "- **Training Epochs**: {training_args.num_train_epochs}\n",
    "- **Learning Rate**: {training_args.learning_rate}\n",
    "- **Dataset Size**: {len(dataset_dict['train'])} examples\n",
    "\n",
    "## Performance\n",
    "\n",
    "- **Training Loss**: {trainer_stats.training_loss:.4f}\n",
    "- **Validation Loss**: {eval_results.get('eval_loss', 'N/A')}\n",
    "\n",
    "## Utilisation\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Chargement du mod√®le\n",
    "model_path = \"votre-username/Qwen3-4B-French-LoRA\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Formatage du prompt\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{{}}\n",
    "\n",
    "### Input:\n",
    "{{}}\n",
    "\n",
    "### Response:\n",
    "{{}}\"\"\"\n",
    "\n",
    "# G√©n√©ration\n",
    "prompt = alpaca_prompt.format(\n",
    "    \"Qu'est-ce que l'intelligence artificielle?\",\n",
    "    \"\",\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Le mod√®le est sp√©cialis√© pour le fran√ßais\n",
    "- Performances limit√©es pour les t√¢ches n√©cessitant un raisonnement complexe\n",
    "- Peut g√©n√©rer des informations incorrectes (hallucinations)\n",
    "\n",
    "## √âthique\n",
    "\n",
    "Ce mod√®le est destin√© √† des fins √©ducatives et de recherche. Les utilisateurs doivent √™tre conscients des biais potentiels et utiliser le mod√®le de mani√®re responsable.\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarde du README\n",
    "with open(model_save_path / \"README.md\", 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"üìù README cr√©√© pour Hugging Face Hub\")\n",
    "\n",
    "# Instructions pour la publication\n",
    "print(\"\\nüöÄ Instructions pour la publication sur Hugging Face Hub:\")\n",
    "print(\"1. Cr√©ez un compte sur Hugging Face: https://huggingface.co/join\")\n",
    "print(\"2. Installez l'CLI Hugging Face: pip install 'huggingface_hub[cli]'\")\n",
    "print(\"3. Connectez-vous: huggingface-cli login\")\n",
    "print(\"4. Cr√©ez un nouveau repository: huggingface-cli repo create Qwen3-4B-French-LoRA --type model\")\n",
    "print(\"5. Uploadez votre mod√®le: huggingface-cli upload ./models/lora_qwen3_french votre-username/Qwen3-4B-French-LoRA\")\n",
    "\n",
    "# Script de publication automatis√©\n",
    "publish_script = f\"\"\"#!/bin/bash\n",
    "# Script pour publier le mod√®le sur Hugging Face Hub\n",
    "\n",
    "# Remplacez VOTRE_USERNAME par votre username Hugging Face\n",
    "USERNAME=\"VOTRE_USERNAME\"\n",
    "MODEL_NAME=\"Qwen3-4B-French-LoRA\"\n",
    "MODEL_PATH=\"./models/lora_qwen3_french\"\n",
    "\n",
    "echo \"Publication du mod√®le $MODEL_NAME sur Hugging Face Hub...\"\n",
    "\n",
    "# V√©rification que le mod√®le existe\n",
    "if [ ! -d \"$MODEL_PATH\" ]; then\n",
    "    echo \"Erreur: Le mod√®le n'existe pas dans $MODEL_PATH\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Upload du mod√®le\n",
    "huggingface-cli upload $MODEL_PATH $USERNAME/$MODEL_NAME \\\n",
    "    --repo-type model \\\n",
    "    --private  # Changez √† --public pour un mod√®le public\n",
    "\n",
    "echo \"‚úÖ Mod√®le publi√© avec succ√®s!\"\n",
    "echo \"üîó Visitez: https://huggingface.co/$USERNAME/$MODEL_NAME\"\n",
    "\"\"\"\n",
    "\n",
    "with open(model_save_path / \"publish.sh\", 'w') as f:\n",
    "    f.write(publish_script)\n",
    "\n",
    "# Rendre le script ex√©cutable\n",
    "os.chmod(model_save_path / \"publish.sh\", 0o755)\n",
    "print(\"\\nüìú Script de publication cr√©√©: publish.sh\")"
   ],
   "id": "9f233d03f8940db3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. R√©sum√© et conclusions"
   ],
   "id": "c09dc8c799895d5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© final\n",
    "print(\"üéâ TP de fine-tuning LoRA termin√©!\\n\")\n",
    "print(\"üìä R√©sum√© des r√©sultats:\")\n",
    "print(f\"   - Mod√®le de base: {model_name}\")\n",
    "print(f\"   - Param√®tres LoRA: rank={lora_config['r']}, alpha={lora_config['lora_alpha']}\")\n",
    "print(f\"   - Dataset d'entra√Ænement: {len(dataset_dict['train'])} exemples\")\n",
    "print(f\"   - √âpoques d'entra√Ænement: {training_args.num_train_epochs}\")\n",
    "print(f\"   - Perte finale: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"   - Perte validation: {eval_results.get('eval_loss', 'N/A')}\")\n",
    "print(f\"   - M√©moire GPU utilis√©e: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"\\nüìÅ Fichiers cr√©√©s:\")\n",
    "print(f\"   - {model_save_path.name}/ (mod√®le LoRA)\")\n",
    "print(f\"   - {model_save_path.name}/merged_16bit/ (mod√®le fusionn√©)\")\n",
    "print(f\"   - {model_save_path.name}/README.md (documentation)\")\n",
    "print(f\"   - {model_save_path.name}/publish.sh (script de publication)\")\n",
    "\n",
    "print(\"\\nüîç Points cl√©s appris:\")\n",
    "print(\"   1. LoRA permet de fine-tuner avec ~1% des param√®tres\")\n",
    "print(\"   2. QLoRA r√©duit significativement la m√©moire GPU\")\n",
    "print(\"   3. Unsloth acc√©l√®re l'entra√Ænement de 2-3x\")\n",
    "print(\"   4. Le mod√®le reste petit et facile √† partager\")\n",
    "\n",
    "print(\"\\nüöÄ Prochaines √©tapes:\")\n",
    "print(\"   1. Publier le mod√®le sur Hugging Face Hub\")\n",
    "print(\"   2. Tester avec des donn√©es r√©elles\")\n",
    "print(\"   3. Exp√©rimenter avec diff√©rents param√®tres LoRA\")\n",
    "print(\"   4. Essayer d'autres techniques de fine-tuning\")"
   ],
   "id": "8f91bcfbb09fa122"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Pour aller plus loin\n",
    "\n",
    "### Exp√©rimentations sugg√©r√©es\n",
    "\n",
    "1. **Variation du rank LoRA**: Tester r=8, r=16, r=32 et comparer les performances\n",
    "2. **Diff√©rents taux d'apprentissage**: 1e-4, 2e-4, 5e-4\n",
    "3. **Augmentation du dataset**: G√©n√©rer plus de donn√©es synth√©tiques\n",
    "4. **Techniques avanc√©es**:\n",
    "   - DoRA (Weight-Decomposed LoRA)\n",
    "   - VeRA (Vector-based Random Matrix Adaptation)\n",
    "   - QLoRA avec diff√©rents niveaux de quantification\n",
    "\n",
    "### Optimisations possibles\n",
    "\n",
    "1. **Batch processing**: Augmenter la taille du batch si m√©moire permet\n",
    "2. **Gradient checkpointing**: Activ√© par d√©faut dans Unsloth\n",
    "3. **Mixed precision**: Utiliser bf16 si disponible\n",
    "4. **Data parallelism**: Pour multi-GPU\n",
    "\n",
    "### √âvaluation approfondie\n",
    "\n",
    "1. **Benchmarks automatiques**: Utiliser des benchmarks fran√ßais\n",
    "2. **√âvaluation humaine**: Qualit√© des r√©ponses g√©n√©r√©es\n",
    "3. **Comparaison baseline**: Contre le mod√®le de base\n",
    "4. **Analyse d'erreurs**: Comprendre les faiblesses du mod√®le\n",
    "\n",
    "### D√©ploiement\n",
    "\n",
    "1. **API REST**: Cr√©er une API pour le mod√®le\n",
    "2. **Optimisation inf√©rence**: vLLM, TensorRT-LLM\n",
    "3. **Quantification post-entra√Ænement**: GGUF, AWQ\n",
    "4. **Monitoring**: Suivi des performances en production"
   ],
   "id": "c9bf101e7d4fbd00"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}