{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Partie 2 : Fine-tuning de LLM avec LoRA\n",
    "\n",
    "## üìö Contexte\n",
    "\n",
    "Dans ce notebook, nous allons explorer le fine-tuning de mod√®les de langage avec la technique LoRA (Low-Rank Adaptation). Cette approche permet d'adapter des LLMs pour des t√¢ches sp√©cifiques.\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "1. **Comprendre** le fonctionnement de LoRA et ses avantages\n",
    "2. **Pr√©parer** le dataset synth√©tique g√©n√©r√© pr√©c√©demment\n",
    "3. **Configurer** un entra√Ænement LoRA avec Unsloth\n",
    "4. **Entra√Æner** un mod√®le Qwen3-4B sp√©cialis√© en fran√ßais\n",
    "5. **√âvaluer** les performances du mod√®le fine-tun√©\n",
    "6. **Publier** le mod√®le sur Hugging Face Hub\n",
    "\n",
    "## üîß Etapes\n",
    "\n",
    "Notre approche utilise :\n",
    "\n",
    "1. **Unsloth** : Biblioth√®que optimis√©e pour le fine-tuning LoRA\n",
    "2. **Qwen3-4B** : Mod√®le de base pr√©-entra√Æn√©\n",
    "3. **QLoRA** : Quantization + LoRA pour r√©duire la m√©moire GPU\n",
    "4. **Hugging Face Hub** : Publication et partage du mod√®le\n",
    "\n",
    "Resources √† garder sous la main durant le TP:\n",
    "\n",
    "- [LoRA Hyperparameters Guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide)\n",
    "- [What Model Should I Use?](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/what-model-should-i-use)\n",
    "- [Qwen3 instruct fine tune guide](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-Instruct.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T14:04:43.815441Z",
     "start_time": "2025-09-02T14:04:43.807337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "‚úÖ Toutes les biblioth√®ques ont √©t√© import√©es avec succ√®s!\n",
      "Device utilis√©: cuda\n",
      "GPU: NVIDIA L4\n",
      "M√©moire GPU: 23.7 GB\n"
     ]
    }
   ],
   "source": [
    "# Import des biblioth√®ques\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Imports Unsloth\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    from trl import SFTTrainer\n",
    "    import trackio\n",
    "    \n",
    "    print(\"‚úÖ Toutes les biblioth√®ques ont √©t√© import√©es avec succ√®s!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erreur d'import: {e}\")\n",
    "    print(\"Veuillez ex√©cuter 'uv sync' pour installer les d√©pendances.\")\n",
    "    raise\n",
    "\n",
    "# V√©rification GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device utilis√©: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "# Configuration visuelle\n",
    "sns.set_theme()\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement et pr√©paration du dataset\n",
    "\n",
    "Nous chargeons le dataset synth√©tique g√©n√©r√© dans la partie pr√©c√©dente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset synth√©tique...\n",
      "‚úÖ Dataset charg√©: 36 paires instruction-r√©ponse\n",
      "\n",
      "üìä Statistiques du dataset:\n",
      "   - Nombre total de paires: 36\n",
      "   - Longueur moyenne des instructions: 102 caract√®res\n",
      "   - Longueur moyenne des r√©ponses: 919 caract√®res\n",
      "\n",
      "üìù Exemples de donn√©es:\n",
      "\n",
      "--- Exemple 1 ---\n",
      "Instruction: En quelle ann√©e a eu lieu la consultation de l'√©lection √©tatique de Katanning ?\n",
      "R√©ponse: La consultation de l'√©lection √©tatique de Katanning a eu lieu en **1935**....\n",
      "\n",
      "--- Exemple 2 ---\n",
      "Instruction: Quel est le nom du cat√©gory de Wikimedia qui regroupe les icefalls de la r√©gion de Ross Dependency ?\n",
      "R√©ponse: Le nom du cat√©gory de Wikimedia qui regroupe les icefalls de la r√©gion de Ross Dependency est **Cate...\n",
      "\n",
      "--- Exemple 3 ---\n",
      "Instruction: Dans quel film Bourvil joue-t-il son dernier r√¥le, selon Jean Pierre Melville, et en quelle ann√©e a-t-il √©t√© tourn√© ?\n",
      "R√©ponse: Selon Jean Pierre Melville, Bourvil joue son dernier r√¥le dans le film **\"Le cercle rouge\"**, qui a ...\n"
     ]
    }
   ],
   "source": [
    "# Configuration des chemins\n",
    "data_dir = Path(\"../data/synthetic\")\n",
    "\n",
    "# Chargement du dataset\n",
    "print(\"Chargement du dataset synth√©tique...\")\n",
    "if (data_dir / \"synthetic_dataset_alpaca.json\").exists():\n",
    "    with open(data_dir / \"synthetic_dataset_alpaca.json\", 'r', encoding='utf-8') as f:\n",
    "        alpaca_data = json.load(f)\n",
    "    print(f\"‚úÖ Dataset charg√©: {len(alpaca_data)} paires instruction-r√©ponse\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset non trouv√©. Veuillez ex√©cuter le notebook 01 d'abord.\")\n",
    "    # Cr√©ation d'un dataset d'exemple\n",
    "    alpaca_data = [\n",
    "        {\n",
    "            \"instruction\": \"Qu'est-ce que l'apprentissage automatique?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"L'apprentissage automatique est un sous-domaine de l'intelligence artificielle qui permet aux syst√®mes d'apprendre √† partir de donn√©es sans √™tre explicitement programm√©s.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Conversion en DataFrame pour analyse\n",
    "df = pd.DataFrame(alpaca_data)\n",
    "print(f\"\\nüìä Statistiques du dataset:\")\n",
    "print(f\"   - Nombre total de paires: {len(df)}\")\n",
    "print(f\"   - Longueur moyenne des instructions: {df['instruction'].str.len().mean():.0f} caract√®res\")\n",
    "print(f\"   - Longueur moyenne des r√©ponses: {df['output'].str.len().mean():.0f} caract√®res\")\n",
    "\n",
    "# Affichage des premi√®res lignes\n",
    "print(\"\\nüìù Exemples de donn√©es:\")\n",
    "for i in range(min(3, len(alpaca_data))):\n",
    "    print(f\"\\n--- Exemple {i+1} ---\")\n",
    "    print(f\"Instruction: {alpaca_data[i]['instruction']}\")\n",
    "    print(f\"R√©ponse: {alpaca_data[i]['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pr√©paration des donn√©es pour l'entra√Ænement\n",
    "\n",
    "Nous formatons les donn√©es selon le format attendu par Unsloth pour le fine-tuning instructionnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatage des donn√©es pour l'entra√Ænement...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4892f5b859a42f2a7cfb8a75ff047e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset pr√©par√©:\n",
      "   - Training set: 32 exemples\n",
      "   - Validation set: 4 exemples\n",
      "\n",
      "üìù Exemple format√©:\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "O√π se trouve le Barnes Ridge et dans quel √©tat est-il situ√© ?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Le Barnes Ridge se trouve dans le **state de Montana**, aux √âtats-Unis. Il est situ√© dans le **comt√© de Fergus County**, dans la partie centrale du pays, √† environ 300 km au sud-ouest de Washington, D.C.<|im_end|>...\n"
     ]
    }
   ],
   "source": [
    "# Template de formatage pour le mod√®le - align√© avec le format Unsloth/Qwen3\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# Le EOS_TOKEN sera d√©fini apr√®s le chargement du tokenizer\n",
    "EOS_TOKEN = \"<|im_end|>\"  # Token de fin pour Qwen3\n",
    "\n",
    "def format_prompts(examples):\n",
    "    \"\"\"\n",
    "    Formate les exemples selon le template Alpaca pour le fine-tuning.\n",
    "    \"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    \n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        # Formatage du prompt\n",
    "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Conversion en Dataset Hugging Face\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Application du formatage\n",
    "print(\"Formatage des donn√©es pour l'entra√Ænement...\")\n",
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "\n",
    "# Split train/validation\n",
    "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test']\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset pr√©par√©:\")\n",
    "print(f\"   - Training set: {len(dataset_dict['train'])} exemples\")\n",
    "print(f\"   - Validation set: {len(dataset_dict['validation'])} exemples\")\n",
    "\n",
    "# Affichage d'un exemple format√©\n",
    "print(\"\\nüìù Exemple format√©:\")\n",
    "print(dataset_dict['train'][0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration du mod√®le et de LoRA avec TrackIO\n",
    "\n",
    "Nous configurons le mod√®le Qwen3-4B avec les param√®tres LoRA optimis√©s selon les meilleures pratiques Unsloth, et initialisons TrackIO pour le suivi des exp√©riences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Trackio project initialized: llm-finetuning-tp\n",
      "* Trackio metrics logged to: /root/.cache/huggingface/trackio\n",
      "* View dashboard by running in your terminal:\n",
      "\u001b[1m\u001b[93mtrackio show --project \"llm-finetuning-tp\"\u001b[0m\n",
      "* or by running in Python: trackio.show(project=\"llm-finetuning-tp\")\n",
      "üìä TrackIO initialis√© - Exp√©rience: qwen3-4b-lora-french-1756829650\n",
      "Chargement du mod√®le Qwen/Qwen3-4B-Instruct-2507...\n",
      "==((====))==  Unsloth 2025.8.10: Fast Qwen3 patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.045 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "‚úÖ Mod√®le charg√© avec succ√®s!\n",
      "   - M√©moire utilis√©e: 3.6 GB\n",
      "   - M√©moire r√©serv√©e: 4.1 GB\n"
     ]
    }
   ],
   "source": [
    "# Initialisation de TrackIO pour le suivi des exp√©riences\n",
    "experiment_name = f\"qwen3-4b-lora-french-{int(time.time())}\"\n",
    "trackio.init(\n",
    "    project=\"llm-finetuning-tp\",\n",
    "    name=experiment_name,\n",
    "    config={\n",
    "        \"model\": \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "        \"dataset\": \"synthetic_french\",\n",
    "        \"framework\": \"unsloth\",\n",
    "        \"optimization\": \"qlora\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"üìä TrackIO initialis√© - Exp√©rience: {experiment_name}\")\n",
    "\n",
    "# Configuration du mod√®le avec param√®tres optimis√©s Unsloth\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "max_seq_length = 2048  # Longueur maximale de s√©quence\n",
    "dtype = None  # D√©tection automatique du type\n",
    "load_in_4bit = True  # Utilisation de la quantification 4-bit\n",
    "\n",
    "print(f\"Chargement du mod√®le {model_name}...\")\n",
    "\n",
    "# Chargement du mod√®le avec Unsloth\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    token=None,  # Pas besoin de token pour les mod√®les publics\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© avec succ√®s!\")\n",
    "print(f\"   - M√©moire utilis√©e: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "print(f\"   - M√©moire r√©serv√©e: {torch.cuda.memory_reserved() / 1e9:.1f} GB\")\n",
    "\n",
    "# Mettre √† jour le EOS_TOKEN avec le tokenizer r√©el\n",
    "global EOS_TOKEN\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Log des informations syst√®me\n",
    "trackio.log({\n",
    "    \"gpu_memory_allocated_gb\": torch.cuda.memory_allocated() / 1e9,\n",
    "    \"gpu_memory_reserved_gb\": torch.cuda.memory_reserved() / 1e9,\n",
    "    \"model_parameters_total\": sum(p.numel() for p in model.parameters()),\n",
    "    \"model_size_gb\": sum(p.numel() * p.element_size() for p in model.parameters()) / 1e9\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration LoRA:\n",
      "   - r: 16\n",
      "   - target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "   - lora_alpha: 16\n",
      "   - lora_dropout: 0\n",
      "   - bias: none\n",
      "   - use_gradient_checkpointing: unsloth\n",
      "   - random_state: 3407\n",
      "   - use_rslora: False\n",
      "   - loftq_config: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.10 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Configuration LoRA appliqu√©e!\n",
      "trainable params: 33,030,144 || all params: 4,055,498,240 || trainable%: 0.8145\n",
      "   - Param√®tres entra√Ænables: None\n"
     ]
    }
   ],
   "source": [
    "# Configuration des param√®tres LoRA optimis√©s selon les best practuces Unsloth\n",
    "# Bas√© sur: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-Instruct.ipynb\n",
    "lora_config = {\n",
    "    \"r\": 16,  # Rank des matrices de mise √† jour\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],  # Modules √† adapter - optimis√© pour Qwen3\n",
    "    \"lora_alpha\": 16,  # Facteur d'√©chelle (align√© avec le rank)\n",
    "    \"lora_dropout\": 0,  # Pas de dropout pour une meilleure stabilit√©\n",
    "    \"bias\": \"none\",  # Pas d'adaptation des biais\n",
    "    \"use_gradient_checkpointing\": \"unsloth\",  # Optimis√© Unsloth\n",
    "    \"random_state\": 3407,\n",
    "    \"use_rslora\": False,  # Ne pas utiliser scaled LoRA\n",
    "    \"loftq_config\": None,  # Configuration LoftQ (pour quantification avanc√©e)\n",
    "}\n",
    "\n",
    "print(\"Configuration LoRA:\")\n",
    "for key, value in lora_config.items():\n",
    "    print(f\"   - {key}: {value}\")\n",
    "\n",
    "# Application de la configuration LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    **lora_config\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Configuration LoRA appliqu√©e!\")\n",
    "\n",
    "# Log des param√®tres LoRA\n",
    "trackio.log({\n",
    "    \"lora_rank\": lora_config[\"r\"],\n",
    "    \"lora_alpha\": lora_config[\"lora_alpha\"],\n",
    "    \"lora_dropout\": lora_config[\"lora_dropout\"],\n",
    "    \"target_modules\": lora_config[\"target_modules\"],\n",
    "    \"gradient_checkpointing\": lora_config[\"use_gradient_checkpointing\"]\n",
    "})\n",
    "\n",
    "# Affichage des param√®tres entra√Ænables\n",
    "trainable_stats = model.print_trainable_parameters()\n",
    "print(f\"   - Param√®tres entra√Ænables: {trainable_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration de l'entra√Ænement\n",
    "\n",
    "Nous configurons les hyperparam√®tres d'entra√Ænement et le trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments d'entra√Ænement:\n",
      "   - Batch size: 2\n",
      "   - Gradient accumulation: 4\n",
      "   - Learning rate: 0.0002\n",
      "   - Epochs: 3\n",
      "   - Warmup steps: 5\n",
      "   - Report to: ['trackio']\n"
     ]
    }
   ],
   "source": [
    "# Configuration des arguments d'entra√Ænement optimis√©s avec TrackIO\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetuned_model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    num_train_epochs=3,  # Nombre d'√©poques\n",
    "    learning_rate=2e-4,  # Taux d'apprentissage optimal pour LoRA\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    report_to=\"trackio\",  # Utilisation de TrackIO pour le tracking\n",
    "    # Param√®tres suppl√©mentaires pour le monitoring\n",
    "    logging_first_step=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    ddp_find_unused_parameters=False,\n",
    ")\n",
    "\n",
    "print(\"Arguments d'entra√Ænement:\")\n",
    "print(f\"   - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   - Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   - Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"   - Report to: {training_args.report_to}\")\n",
    "\n",
    "# Log des hyperparam√®tres\n",
    "trackio.log({\n",
    "    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"num_train_epochs\": training_args.num_train_epochs,\n",
    "    \"warmup_steps\": training_args.warmup_steps,\n",
    "    \"weight_decay\": training_args.weight_decay,\n",
    "    \"optimizer\": training_args.optim,\n",
    "    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n",
    "    \"effective_batch_size\": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930fb34b813c4c509782b7c7d0a3860b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=12):   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n",
      "WARNING:datasets.arrow_dataset:num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7f3c625af54cd3b49f9300d7b40ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SFTTrainer configur√©!\n",
      "   - √âchantillons d'entra√Ænement: 32\n",
      "   - √âchantillons de validation: 4\n",
      "   - Sequence length: 2048\n",
      "   - Packing: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration du trainer SFT de TRL\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"validation\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Ne pas packer les s√©quences pour Qwen3\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SFTTrainer configur√©!\")\n",
    "print(f\"   - √âchantillons d'entra√Ænement: {len(trainer.train_dataset)}\")\n",
    "print(f\"   - √âchantillons de validation: {len(trainer.eval_dataset)}\")\n",
    "print(f\"   - Sequence length: {max_seq_length}\")\n",
    "print(f\"   - Packing: {False}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entra√Ænement du mod√®le\n",
    "\n",
    "**‚ö†Ô∏è Important** : Cette √©tape peut prendre du temps (15-30 minutes) selon votre GPU et la taille du dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©marrage de l'entra√Ænement LoRA...\n",
      "   - Dataset: 32 exemples\n",
      "   - Param√®tres LoRA: rank=16, alpha=16\n",
      "   - Learning rate: 0.0002\n",
      "   - Epochs: 3\n",
      "   - TrackIO: Activ√©\\n\n",
      "M√©moire GPU avant entra√Ænement: 3.7 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 32 | Num Epochs = 3 | Total steps = 12\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Trackio project initialized: huggingface\n",
      "* Trackio metrics logged to: /root/.cache/huggingface/trackio\n",
      "* View dashboard by running in your terminal:\n",
      "\u001b[1m\u001b[93mtrackio show --project \"huggingface\"\u001b[0m\n",
      "* or by running in Python: trackio.show(project=\"huggingface\")\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.510600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.292400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.350100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.352300</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.031100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.001400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.956400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.062900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.884900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.820600</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.936200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Run finished. Uploading logs to Trackio Space: http://127.0.0.1:7860/ (please wait...)\n",
      "\\n‚úÖ Entra√Ænement termin√©!\n",
      "   - Perte finale: 1.1228\n",
      "   - Temps d'entra√Ænement: 38.5 secondes\n",
      "M√©moire GPU apr√®s entra√Ænement: 3.8 GB\n"
     ]
    }
   ],
   "source": [
    "# D√©marrage de l'entra√Ænement avec monitoring TrackIO\n",
    "print(\"üöÄ D√©marrage de l'entra√Ænement LoRA...\")\n",
    "print(f\"   - Dataset: {len(dataset_dict['train'])} exemples\")\n",
    "print(f\"   - Param√®tres LoRA: rank={lora_config['r']}, alpha={lora_config['lora_alpha']}\")\n",
    "print(f\"   - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   - TrackIO: Activ√©\\\\n\")\n",
    "\n",
    "# Suivi de la m√©moire avant entra√Ænement\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"M√©moire GPU avant entra√Ænement: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "\n",
    "# Fonction de monitoring GPU pour TrackIO\n",
    "def log_gpu_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_stats = {\n",
    "            \"gpu_memory_allocated_gb\": torch.cuda.memory_allocated() / 1e9,\n",
    "            \"gpu_memory_reserved_gb\": torch.cuda.memory_reserved() / 1e9,\n",
    "            \"gpu_utilization_percent\": torch.cuda.utilization(),\n",
    "        }\n",
    "        try:\n",
    "            # Tentative de lecture de la temp√©rature et de la puissance\n",
    "            gpu_stats.update({\n",
    "                \"gpu_temperature_c\": torch.cuda.temperature(),\n",
    "                \"gpu_power_watts\": torch.cuda.power_draw(),\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "        trackio.log(gpu_stats)\n",
    "\n",
    "# Log initial\n",
    "log_gpu_stats()\n",
    "trackio.log({\n",
    "    \"training_start_time\": time.time(),\n",
    "    \"dataset_size\": len(dataset_dict['train']),\n",
    "    \"validation_size\": len(dataset_dict['validation'])\n",
    "})\n",
    "\n",
    "# Lancement de l'entra√Ænement\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\\\n‚úÖ Entra√Ænement termin√©!\")\n",
    "print(f\"   - Perte finale: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"   - Temps d'entra√Ænement: {trainer_stats.metrics.get('train_runtime', 0):.1f} secondes\")\n",
    "\n",
    "# Suivi de la m√©moire apr√®s entra√Ænement\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"M√©moire GPU apr√®s entra√Ænement: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "\n",
    "# Log des r√©sultats finaux\n",
    "log_gpu_stats()\n",
    "trackio.log({\n",
    "    \"training_end_time\": time.time(),\n",
    "    \"final_training_loss\": trainer_stats.training_loss,\n",
    "    \"training_runtime_seconds\": trainer_stats.metrics.get('train_runtime', 0),\n",
    "    \"training_samples_per_second\": trainer_stats.metrics.get('train_samples_per_second', 0),\n",
    "    \"total_steps\": trainer.state.global_step\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. √âvaluation du mod√®le\n",
    "\n",
    "Nous √©valuons les performances du mod√®le fine-tun√© sur le jeu de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä √âvaluation du mod√®le sur le jeu de validation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nR√©sultats de l'√©valuation:\n",
      "   - eval_loss: 1.1588\n",
      "   - eval_runtime: 0.4112\n",
      "   - eval_samples_per_second: 9.7280\n",
      "   - eval_steps_per_second: 4.8640\n",
      "   - epoch: 3.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# √âvaluation sur le jeu de validation avec TrackIO\n",
    "print(\"üìä √âvaluation du mod√®le sur le jeu de validation...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\\\nR√©sultats de l'√©valuation:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   - {key}: {value:.4f}\")\n",
    "\n",
    "# Log des r√©sultats d'√©valuation\n",
    "trackio.log({\n",
    "    \"eval_loss\": eval_results.get(\"eval_loss\", 0),\n",
    "    \"eval_runtime\": eval_results.get(\"eval_runtime\", 0),\n",
    "    \"eval_samples_per_second\": eval_results.get(\"eval_samples_per_second\", 0),\n",
    "    \"eval_steps_per_second\": eval_results.get(\"eval_steps_per_second\", 0)\n",
    "})\n",
    "\n",
    "# Visualisation de la courbe d'apprentissage\n",
    "if hasattr(trainer, \"state\") and trainer.state.log_history:\n",
    "    log_history = trainer.state.log_history\n",
    "    losses = [log.get('loss', 0) for log in log_history if 'loss' in log]\n",
    "    steps = [log.get('step', 0) for log in log_history if 'loss' in log]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(steps, losses, 'b-', label='Training Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Courbe d\\\\apprentissage')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Log des m√©triques de la courbe d'apprentissage\n",
    "    if losses:\n",
    "        trackio.log({\n",
    "            \"training_loss_min\": min(losses),\n",
    "            \"training_loss_final\": losses[-1],\n",
    "            \"training_loss_improvement\": losses[0] - losses[-1] if len(losses) > 1 else 0\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test d'inf√©rence\n",
    "\n",
    "Nous testons le mod√®le fine-tun√© sur quelques exemples pour √©valuer sa capacit√© √† g√©n√©rer des r√©ponses en fran√ßais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Tests d'inf√©rence:\n",
      "\n",
      "--- Test 1 ---\n",
      "Instruction: Qu'est-ce que l'intelligence artificielle?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInstruction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m response = \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m inference_time = time.time() - start_time\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mR√©ponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[:\u001b[32m300\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mgenerate_response\u001b[39m\u001b[34m(instruction, max_new_tokens)\u001b[39m\n\u001b[32m     16\u001b[39m inputs = tokenizer([prompt], return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# G√©n√©ration avec param√®tres optimis√©s pour le fran√ßais\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Utilisation des optimisations Unsloth\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Activ√© par d√©faut avec Unsloth\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m response = tokenizer.batch_decode(outputs)[\u001b[32m0\u001b[39m]\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Extraire seulement la r√©ponse g√©n√©r√©e\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TP-LLM-finetuning-centrale/.venv/lib/python3.13/site-packages/peft/peft_model.py:1973\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1971\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1972\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1973\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1974\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1975\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TP-LLM-finetuning-centrale/.venv/lib/python3.13/site-packages/unsloth/models/llama.py:1795\u001b[39m, in \u001b[36munsloth_fast_generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1793\u001b[39m \u001b[38;5;66;03m# Mixed precision autocast\u001b[39;00m\n\u001b[32m   1794\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode(), torch.autocast(device_type = DEVICE_TYPE, dtype = dtype):\n\u001b[32m-> \u001b[39m\u001b[32m1795\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1798\u001b[39m \u001b[38;5;66;03m# Return accelerate back\u001b[39;00m\n\u001b[32m   1799\u001b[39m \u001b[38;5;66;03m# if accelerate_new_send_to_device is not None:\u001b[39;00m\n\u001b[32m   1800\u001b[39m \u001b[38;5;66;03m#     accelerate.utils.operations.send_to_device = accelerate_old_send_to_device\u001b[39;00m\n\u001b[32m   1801\u001b[39m \u001b[38;5;66;03m# pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TP-LLM-finetuning-centrale/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TP-LLM-finetuning-centrale/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2539\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin.generate(\n\u001b[32m   2529\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2530\u001b[39m         inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2534\u001b[39m         **kwargs,\n\u001b[32m   2535\u001b[39m     )\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m   2551\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._beam_search(\n\u001b[32m   2552\u001b[39m         input_ids,\n\u001b[32m   2553\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2557\u001b[39m         **model_kwargs,\n\u001b[32m   2558\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TP-LLM-finetuning-centrale/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2867\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2864\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   2866\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2867\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2868\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2869\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TP-LLM-finetuning-centrale/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TP-LLM-finetuning-centrale/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TP-LLM-finetuning-centrale/.venv/lib/python3.13/site-packages/unsloth/models/llama.py:1167\u001b[39m, in \u001b[36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[39m\n\u001b[32m   1149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_CausalLM_fast_forward\u001b[39m(\n\u001b[32m   1150\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1151\u001b[39m     input_ids: torch.LongTensor = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1164\u001b[39m     *args, **kwargs,\n\u001b[32m   1165\u001b[39m ) -> Union[Tuple, CausalLMOutputWithPast]:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m         outputs = \u001b[43mfast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1174\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1175\u001b[39m         causal_mask = xformers.attn_bias.LowerTriangularMask() \u001b[38;5;28;01mif\u001b[39;00m HAS_XFORMERS \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TP-LLM-finetuning-centrale/.venv/lib/python3.13/site-packages/unsloth/models/llama.py:1072\u001b[39m, in \u001b[36m_LlamaModel_fast_forward_inference.<locals>.LlamaModel_fast_forward_inference_custom\u001b[39m\u001b[34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[39m\n\u001b[32m   1069\u001b[39m temp_mlp = torch.empty((\u001b[32m2\u001b[39m, bsz, \u001b[32m1\u001b[39m, mlp_size), dtype = X.dtype, device = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEVICE_TYPE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1070\u001b[39m temp_gates, temp_ups = \u001b[38;5;28mtuple\u001b[39m(temp_mlp[\u001b[32m0\u001b[39m].to(torch.device(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(DEVICE_COUNT)), \u001b[38;5;28mtuple\u001b[39m(temp_mlp[\u001b[32m1\u001b[39m].to(torch.device(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(DEVICE_COUNT))\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m seq_len = \u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m[-\u001b[32m2\u001b[39m]\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bsz != \u001b[32m1\u001b[39m:\n\u001b[32m   1074\u001b[39m     attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[32m   1075\u001b[39m         attention_mask,\n\u001b[32m   1076\u001b[39m         (bsz, q_len),\n\u001b[32m   (...)\u001b[39m\u001b[32m   1079\u001b[39m         sliding_window = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.config, \u001b[33m\"\u001b[39m\u001b[33msliding_window\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1080\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Configuration du mod√®le pour l'inf√©rence optimis√©e avec Unsloth\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Fonction de g√©n√©ration optimis√©e avec Unsloth\n",
    "def generate_response(instruction: str, max_new_tokens: int = 512) -> str:\n",
    "    \"\"\"\n",
    "    G√©n√®re une r√©ponse √† partir d'une instruction avec param√®tres optimis√©s Unsloth.\n",
    "    \"\"\"\n",
    "    # Formatage du prompt selon le template utilis√© pendant l'entra√Ænement\n",
    "    prompt = alpaca_prompt.format(\n",
    "        instruction,\n",
    "        \"\",  # Input vide\n",
    "        \"\"  # Laisser vide pour la g√©n√©ration\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # G√©n√©ration avec param√®tres optimis√©s pour le fran√ßais\n",
    "    # Utilisation des optimisations Unsloth\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,  # Activ√© par d√©faut avec Unsloth\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    # Extraire seulement la r√©ponse g√©n√©r√©e\n",
    "    response = response.split(\"### Response:\")[1].strip()\n",
    "    return response\n",
    "\n",
    "# Test avec exemples vari√©s en fran√ßais\n",
    "test_instructions = [\n",
    "    \"Qu'est-ce que l'intelligence artificielle?\",\n",
    "    \"Explique le principe du fine-tuning avec LoRA.\",\n",
    "    \"Quels sont les avantages de QLoRA par rapport au fine-tuning classique?\",\n",
    "    \"Comment fonctionne l'apprentissage par renforcement?\",\n",
    "    \"Qu'est-ce que la quantification 4-bit dans les LLMs?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Tests d'inf√©rence:\\n\")\n",
    "\n",
    "# Log des tests d'inf√©rence\n",
    "inference_results = []\n",
    "\n",
    "for i, instruction in enumerate(test_instructions):\n",
    "    print(f\"--- Test {i+1} ---\")\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = generate_response(instruction)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"R√©ponse: {response[:300]}...\")\n",
    "    print(f\"Temps de g√©n√©ration: {inference_time:.2f} secondes\\n\")\n",
    "    \n",
    "    # Stocker les r√©sultats pour le logging\n",
    "    inference_results.append({\n",
    "        \"test_id\": i + 1,\n",
    "        \"instruction\": instruction,\n",
    "        \"response_length\": len(response),\n",
    "        \"inference_time\": inference_time,\n",
    "        \"tokens_per_second\": len(response.split()) / inference_time if inference_time > 0 else 0\n",
    "    })\n",
    "\n",
    "# Log des r√©sultats d'inf√©rence\n",
    "trackio.log({\n",
    "    \"inference_tests\": inference_results,\n",
    "    \"avg_inference_time\": sum(r[\"inference_time\"] for r in inference_results) / len(inference_results),\n",
    "    \"avg_tokens_per_second\": sum(r[\"tokens_per_second\"] for r in inference_results) / len(inference_results)\n",
    "})\n",
    "\n",
    "# Afficher les statistiques d'inf√©rence\n",
    "print(\"\\nüìä Statistiques d'inf√©rence:\")\n",
    "print(f\"   - Temps moyen: {sum(r['inference_time'] for r in inference_results) / len(inference_results):.2f}s\")\n",
    "print(f\"   - Tokens/sec moyen: {sum(r['tokens_per_second'] for r in inference_results) / len(inference_results):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyse des compromis LoRA\n",
    "\n",
    "Nous analysons les avantages et inconv√©nients de l'approche LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analyse des param√®tres:\n",
      "   - Param√®tres totaux: 2,539,650,560\n",
      "   - Param√®tres entra√Ænables: 33,030,144\n",
      "   - Pourcentage entra√Ænable: 1.30%\n",
      "\n",
      "üîÑ Comparaison des approches:\n",
      "        Approche Param√®tres entra√Ænables M√©moire GPU estim√©e Temps d'entra√Ænement Performance relative\n",
      "Full Fine-tuning           2,539,650,560               ~24GB                 100%                 100%\n",
      "     LoRA (r=16)              33,030,144                ~8GB                 ~30%                 ~95%\n",
      "      LoRA (r=8)              16,515,072                ~6GB                 ~20%                 ~85%\n",
      "     LoRA (r=32)              66,060,288               ~10GB                 ~40%                 ~98%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyse des param√®tres entra√Ænables\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(\"üìä Analyse des param√®tres:\")\n",
    "print(f\"   - Param√®tres totaux: {total_params:,}\")\n",
    "print(f\"   - Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "print(f\"   - Pourcentage entra√Ænable: {percentage:.2f}%\")\n",
    "\n",
    "# Comparaison des approches\n",
    "comparison_data = {\n",
    "    \"Approche\": [\"Full Fine-tuning\", \"LoRA (r=16)\", \"LoRA (r=8)\", \"LoRA (r=32)\"],\n",
    "    \"Param√®tres entra√Ænables\": [\n",
    "        f\"{total_params:,}\",\n",
    "        f\"{trainable_params:,}\",\n",
    "        f\"{trainable_params//2:,}\",\n",
    "        f\"{trainable_params*2:,}\"\n",
    "    ],\n",
    "    \"M√©moire GPU estim√©e\": [\"~24GB\", \"~8GB\", \"~6GB\", \"~10GB\"],\n",
    "    \"Temps d'entra√Ænement\": [\"100%\", \"~30%\", \"~20%\", \"~40%\"],\n",
    "    \"Performance relative\": [\"100%\", \"~95%\", \"~85%\", \"~98%\"]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüîÑ Comparaison des approches:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution des types de param√®tres\n",
    "param_types = ['Entra√Ænables', 'Gel√©s']\n",
    "param_counts = [trainable_params, total_params - trainable_params]\n",
    "\n",
    "axes[0].pie(param_counts, labels=param_types, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Distribution des param√®tres')\n",
    "\n",
    "# Efficacit√© m√©moire\n",
    "approaches = ['Full FT', 'LoRA r=16', 'LoRA r=8', 'LoRA r=32']\n",
    "memory_usage = [100, 33, 25, 42]  # Pourcentage du full fine-tuning\n",
    "\n",
    "axes[1].bar(approaches, memory_usage, color=['red', 'green', 'blue', 'orange'])\n",
    "axes[1].set_ylabel('M√©moire GPU relative (%)')\n",
    "axes[1].set_title('Efficacit√© m√©moire')\n",
    "axes[1].set_ylim(0, 120)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for i, v in enumerate(memory_usage):\n",
    "    axes[1].text(i, v + 2, f'{v}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sauvegarde du mod√®le\n",
    "\n",
    "Nous sauvegardons le mod√®le fine-tun√© pour une utilisation future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Sauvegarde du mod√®le dans ../models/lora_qwen3_french...\n",
      "‚úÖ Adaptateurs LoRA sauvegard√©s avec succ√®s!\n",
      "\n",
      "üíæ Sauvegarde en 16-bit pour l'inf√©rence...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529475af9a1e451f991ce2d63f6e37dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6fe43d238f43e9bd7d33c8b15af52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f37d90d61a4de794f7312e62ca6e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|                                                                                                                                  | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb60df78b4d249f1897de2e864308609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                             | 1/2 [00:30<00:30, 30.27s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba55e89bca0745ca9df29e8687abf142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:45<00:00, 22.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sauvegarde 16-bit termin√©e!\n",
      "\n",
      "üíæ Sauvegarde en 4-bit pour la taille minimale...\n",
      "Unsloth: Merging LoRA weights into 4bit model...\n",
      "Unsloth: Merging finished.\n",
      "Unsloth: Found skipped modules: ['model.layers.0.self_attn.q_proj', 'model.layers.0.self_attn.k_proj', 'model.layers.0.self_attn.v_proj', 'model.layers.0.self_attn.o_proj', 'model.layers.1.mlp.gate_proj', 'model.layers.1.mlp.up_proj', 'model.layers.1.mlp.down_proj', 'model.layers.2.mlp.gate_proj', 'model.layers.2.mlp.up_proj', 'model.layers.2.mlp.down_proj', 'model.layers.3.self_attn.q_proj', 'model.layers.3.self_attn.k_proj', 'model.layers.3.self_attn.v_proj', 'model.layers.3.self_attn.o_proj', 'model.layers.3.mlp.gate_proj', 'model.layers.3.mlp.up_proj', 'model.layers.3.mlp.down_proj', 'model.layers.5.mlp.gate_proj', 'model.layers.5.mlp.up_proj', 'model.layers.5.mlp.down_proj', 'model.layers.6.self_attn.q_proj', 'model.layers.6.self_attn.k_proj', 'model.layers.6.self_attn.v_proj', 'model.layers.6.self_attn.o_proj', 'model.layers.6.mlp.gate_proj', 'model.layers.6.mlp.up_proj', 'model.layers.6.mlp.down_proj', 'model.layers.34.mlp.gate_proj', 'model.layers.34.mlp.up_proj', 'model.layers.34.mlp.down_proj', 'model.layers.35.mlp.gate_proj', 'model.layers.35.mlp.up_proj', 'model.layers.35.mlp.down_proj', 'lm_head']. Updating config.\n",
      "Unsloth: Saving merged 4bit model to ../models/lora_qwen3_french/merged_4bit...\n",
      "Unsloth: Merged 4bit model saved.\n",
      "Unsloth: Merged 4bit model process completed.\n",
      "‚úÖ Sauvegarde 4-bit termin√©e!\n",
      "\n",
      "üìä Tailles des mod√®les sauvegard√©s:\n",
      "   - LoRA adapters: 82.0 MB\n",
      "   - merged_16bit: 11673.0 MB\n",
      "   - merged_4bit: 3561.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation du dossier de sauvegarde\n",
    "model_save_path = Path(\"../models/lora_qwen3_french\")\n",
    "model_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Sauvegarde du mod√®le dans {model_save_path}...\")\n",
    "\n",
    "# Sauvegarde du mod√®le LoRA avec Unsloth\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(\"‚úÖ Adaptateurs LoRA sauvegard√©s avec succ√®s!\")\n",
    "\n",
    "# Sauvegarde de la configuration\n",
    "config = {\n",
    "    \"base_model\": model_name,\n",
    "    \"lora_config\": lora_config,\n",
    "    \"training_args\": {\n",
    "        \"num_train_epochs\": training_args.num_train_epochs,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"train_size\": len(dataset_dict['train']),\n",
    "        \"val_size\": len(dataset_dict['validation']),\n",
    "        \"source\": \"synthetic_dataset\"\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"final_loss\": trainer_stats.training_loss,\n",
    "        \"eval_results\": eval_results\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(model_save_path / \"config.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Sauvegarde en 16-bit pour l'inf√©rence plus rapide\n",
    "print(\"\\nüíæ Sauvegarde en 16-bit pour l'inf√©rence...\")\n",
    "model.save_pretrained_merged(\n",
    "    model_save_path / \"merged_16bit\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\"\n",
    ")\n",
    "print(\"‚úÖ Sauvegarde 16-bit termin√©e!\")\n",
    "\n",
    "# Sauvegarde en 4-bit pour la taille minimale\n",
    "print(\"\\nüíæ Sauvegarde en 4-bit pour la taille minimale...\")\n",
    "model.save_pretrained_merged(\n",
    "    model_save_path / \"merged_4bit\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_4bit_forced\"\n",
    ")\n",
    "print(\"‚úÖ Sauvegarde 4-bit termin√©e!\")\n",
    "\n",
    "# Affichage des tailles des mod√®les\n",
    "print(\"\\nüìä Tailles des mod√®les sauvegard√©s:\")\n",
    "for folder in [\"\", \"merged_16bit\", \"merged_4bit\"]:\n",
    "    folder_path = model_save_path / folder if folder else model_save_path\n",
    "    if folder_path.exists():\n",
    "        size = sum(os.path.getsize(folder_path / f) for f in os.listdir(folder_path) \n",
    "                  if os.path.isfile(folder_path / f)) / 1e6\n",
    "        print(f\"   - {folder if folder else 'LoRA adapters'}: {size:.1f} MB\")\n",
    "\n",
    "# Log des informations de sauvegarde\n",
    "trackio.log({\n",
    "    \"model_saved\": True,\n",
    "    \"save_path\": str(model_save_path),\n",
    "    \"saved_formats\": [\"lora_adapters\", \"merged_16bit\", \"merged_4bit\"],\n",
    "    \"final_training_loss\": trainer_stats.training_loss\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Pr√©paration pour la publication sur Hugging Face\n",
    "\n",
    "Nous pr√©parons le mod√®le pour sa publication sur Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2040911683.py, line 52)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31malpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\u001b[39m\n                       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation du README pour Hugging Face\n",
    "readme_content = f\"\"\"---\n",
    "library_name: transformers\n",
    "license: mit\n",
    "base_model: {model_name}\n",
    "tags:\n",
    "- trl\n",
    "- sft\n",
    "- generated_from_trainer\n",
    "- french\n",
    "- lora\n",
    "- qwen3\n",
    "model-index:\n",
    "- name: Qwen3-4B-French-LoRA\n",
    "  results: []\n",
    "---\n",
    "\n",
    "# Qwen3-4B-French-LoRA\n",
    "\n",
    "## Description\n",
    "\n",
    "Ce mod√®le est une version fine-tun√©e de Qwen3-4B-Instruct sp√©cialis√©e pour le fran√ßais. Il a √©t√© entra√Æn√© sur un dataset synth√©tique d'instructions-r√©ponses en fran√ßais.\n",
    "\n",
    "## D√©tails d'entra√Ænement\n",
    "\n",
    "- **Base Model**: {model_name}\n",
    "- **Training Framework**: Unsloth + TRL\n",
    "- **Quantization**: 4-bit (QLoRA)\n",
    "- **LoRA Rank**: {lora_config['r']}\n",
    "- **LoRA Alpha**: {lora_config['lora_alpha']}\n",
    "- **Training Epochs**: {training_args.num_train_epochs}\n",
    "- **Learning Rate**: {training_args.learning_rate}\n",
    "- **Dataset Size**: {len(dataset_dict['train'])} examples\n",
    "\n",
    "## Performance\n",
    "\n",
    "- **Training Loss**: {trainer_stats.training_loss:.4f}\n",
    "- **Validation Loss**: {eval_results.get('eval_loss', 'N/A')}\n",
    "\n",
    "## Utilisation\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Chargement du mod√®le\n",
    "model_path = \"votre-username/Qwen3-4B-French-LoRA\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Formatage du prompt\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{{}}\n",
    "\n",
    "### Input:\n",
    "{{}}\n",
    "\n",
    "### Response:\n",
    "{{}}\"\"\"\n",
    "\n",
    "# G√©n√©ration\n",
    "prompt = alpaca_prompt.format(\n",
    "    \"Qu'est-ce que l'intelligence artificielle?\",\n",
    "    \"\",\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Le mod√®le est sp√©cialis√© pour le fran√ßais\n",
    "- Performances limit√©es pour les t√¢ches n√©cessitant un raisonnement complexe\n",
    "- Peut g√©n√©rer des informations incorrectes (hallucinations)\n",
    "\n",
    "## √âthique\n",
    "\n",
    "Ce mod√®le est destin√© √† des fins √©ducatives et de recherche. Les utilisateurs doivent √™tre conscients des biais potentiels et utiliser le mod√®le de mani√®re responsable.\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarde du README\n",
    "with open(model_save_path / \"README.md\", 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"üìù README cr√©√© pour Hugging Face Hub\")\n",
    "\n",
    "# Instructions pour la publication\n",
    "print(\"\\nüöÄ Instructions pour la publication sur Hugging Face Hub:\")\n",
    "print(\"1. Cr√©ez un compte sur Hugging Face: https://huggingface.co/join\")\n",
    "print(\"2. Installez l'CLI Hugging Face: pip install 'huggingface_hub[cli]'\")\n",
    "print(\"3. Connectez-vous: huggingface-cli login\")\n",
    "print(\"4. Cr√©ez un nouveau repository: huggingface-cli repo create Qwen3-4B-French-LoRA --type model\")\n",
    "print(\"5. Uploadez votre mod√®le: huggingface-cli upload ./models/lora_qwen3_french votre-username/Qwen3-4B-French-LoRA\")\n",
    "\n",
    "# Script de publication automatis√©\n",
    "publish_script = f\"\"\"#!/bin/bash\n",
    "# Script pour publier le mod√®le sur Hugging Face Hub\n",
    "\n",
    "# Remplacez VOTRE_USERNAME par votre username Hugging Face\n",
    "USERNAME=\"VOTRE_USERNAME\"\n",
    "MODEL_NAME=\"Qwen3-4B-French-LoRA\"\n",
    "MODEL_PATH=\"./models/lora_qwen3_french\"\n",
    "\n",
    "echo \"Publication du mod√®le $MODEL_NAME sur Hugging Face Hub...\"\n",
    "\n",
    "# V√©rification que le mod√®le existe\n",
    "if [ ! -d \"$MODEL_PATH\" ]; then\n",
    "    echo \"Erreur: Le mod√®le n'existe pas dans $MODEL_PATH\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Upload du mod√®le\n",
    "huggingface-cli upload $MODEL_PATH $USERNAME/$MODEL_NAME \\\n",
    "    --repo-type model \\\n",
    "    --private  # Changez √† --public pour un mod√®le public\n",
    "\n",
    "echo \"‚úÖ Mod√®le publi√© avec succ√®s!\"\n",
    "echo \"üîó Visitez: https://huggingface.co/$USERNAME/$MODEL_NAME\"\n",
    "\"\"\"\n",
    "\n",
    "with open(model_save_path / \"publish.sh\", 'w') as f:\n",
    "    f.write(publish_script)\n",
    "\n",
    "# Rendre le script ex√©cutable\n",
    "os.chmod(model_save_path / \"publish.sh\", 0o755)\n",
    "print(\"\\nüìú Script de publication cr√©√©: publish.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. R√©sum√© et conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ TP de fine-tuning LoRA avec Unsloth termin√©!\n",
      "\n",
      "üìä R√©sum√© des r√©sultats:\n",
      "   - Mod√®le de base: Qwen/Qwen3-4B-Instruct-2507\n",
      "   - Param√®tres LoRA: rank=16, alpha=16\n",
      "   - Dataset d'entra√Ænement: 32 exemples\n",
      "   - √âpoques d'entra√Ænement: 3\n",
      "   - Perte finale: 1.1228\n",
      "   - Perte validation: 1.1588257551193237\n",
      "   - M√©moire GPU utilis√©e: 3.8 GB\n",
      "   - TrackIO: Activ√© et logg√©\n",
      "\n",
      "üìÅ Fichiers cr√©√©s:\n",
      "   - lora_qwen3_french/ (adaptateurs LoRA)\n",
      "   - lora_qwen3_french/merged_16bit/ (mod√®le fusionn√© 16-bit)\n",
      "   - lora_qwen3_french/merged_4bit/ (mod√®le fusionn√© 4-bit)\n",
      "   - lora_qwen3_french/README.md (documentation)\n",
      "   - lora_qwen3_french/publish.sh (script de publication)\n",
      "\n",
      "üîç Points cl√©s appris:\n",
      "   1. LoRA permet de fine-tuner avec ~1% des param√®tres\n",
      "   2. QLoRA r√©duit significativement la m√©moire GPU\n",
      "   3. Unsloth acc√©l√®re l'entra√Ænement de 2-3x\n",
      "   4. TrackIO permet un suivi pr√©cis des exp√©riences\n",
      "   5. Plusieurs formats de sauvegarde pour diff√©rents cas d'usage\n",
      "\n",
      "üöÄ Prochaines √©tapes:\n",
      "   1. Publier le mod√®le sur Hugging Face Hub\n",
      "   2. Tester avec des donn√©es r√©elles\n",
      "   3. Exp√©rimenter avec diff√©rents param√®tres LoRA\n",
      "   4. Essayer d'autres techniques de fine-tuning\n",
      "   5. Analyser les r√©sultats dans TrackIO dashboard\n",
      "* Run finished. Uploading logs to Trackio Space: http://127.0.0.1:7860/ (please wait...)\n",
      "\n",
      "üìä TrackIO: Exp√©rience compl√©t√©e et sauvegard√©e!\n",
      "   - Visualisez les r√©sultats avec: trackio show\n",
      "   - Ou consultez le dashboard TrackIO\n"
     ]
    }
   ],
   "source": [
    "# R√©sum√© final et completion de TrackIO\n",
    "print(\"üéâ TP de fine-tuning LoRA avec Unsloth termin√©!\\n\")\n",
    "\n",
    "print(\"üìä R√©sum√© des r√©sultats:\")\n",
    "print(f\"   - Mod√®le de base: {model_name}\")\n",
    "print(f\"   - Param√®tres LoRA: rank={lora_config['r']}, alpha={lora_config['lora_alpha']}\")\n",
    "print(f\"   - Dataset d'entra√Ænement: {len(dataset_dict['train'])} exemples\")\n",
    "print(f\"   - √âpoques d'entra√Ænement: {training_args.num_train_epochs}\")\n",
    "print(f\"   - Perte finale: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"   - Perte validation: {eval_results.get('eval_loss', 'N/A')}\")\n",
    "print(f\"   - M√©moire GPU utilis√©e: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "print(f\"   - TrackIO: Activ√© et logg√©\")\n",
    "\n",
    "print(\"\\nüìÅ Fichiers cr√©√©s:\")\n",
    "print(f\"   - {model_save_path.name}/ (adaptateurs LoRA)\")\n",
    "print(f\"   - {model_save_path.name}/merged_16bit/ (mod√®le fusionn√© 16-bit)\")\n",
    "print(f\"   - {model_save_path.name}/merged_4bit/ (mod√®le fusionn√© 4-bit)\")\n",
    "print(f\"   - {model_save_path.name}/README.md (documentation)\")\n",
    "print(f\"   - {model_save_path.name}/publish.sh (script de publication)\")\n",
    "\n",
    "print(\"\\nüîç Points cl√©s appris:\")\n",
    "print(\"   1. LoRA permet de fine-tuner avec ~1% des param√®tres\")\n",
    "print(\"   2. QLoRA r√©duit significativement la m√©moire GPU\")\n",
    "print(\"   3. Unsloth acc√©l√®re l'entra√Ænement de 2-3x\")\n",
    "print(\"   4. TrackIO permet un suivi pr√©cis des exp√©riences\")\n",
    "print(\"   5. Plusieurs formats de sauvegarde pour diff√©rents cas d'usage\")\n",
    "\n",
    "print(\"\\nüöÄ Prochaines √©tapes:\")\n",
    "print(\"   1. Publier le mod√®le sur Hugging Face Hub\")\n",
    "print(\"   2. Tester avec des donn√©es r√©elles\")\n",
    "print(\"   3. Exp√©rimenter avec diff√©rents param√®tres LoRA\")\n",
    "print(\"   4. Essayer d'autres techniques de fine-tuning\")\n",
    "print(\"   5. Analyser les r√©sultats dans TrackIO dashboard\")\n",
    "\n",
    "# Finalisation de TrackIO\n",
    "trackio.log({\n",
    "    \"experiment_completed\": True,\n",
    "    \"completion_time\": time.time(),\n",
    "    \"model_saved\": True,\n",
    "    \"total_experiment_time\": time.time() - trackio._start_time if hasattr(trackio, '_start_time') else 0,\n",
    "    \"key_learnings\": [\n",
    "        \"Unsloth provides significant speedup for LoRA training\",\n",
    "        \"QLoRA enables training on consumer GPUs\",\n",
    "        \"Multiple save formats provide flexibility for deployment\",\n",
    "        \"TrackIO enables experiment tracking and comparison\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Completion de l'exp√©rience\n",
    "trackio.finish()\n",
    "\n",
    "print(\"\\nüìä TrackIO: Exp√©rience compl√©t√©e et sauvegard√©e!\")\n",
    "print(\"   - Visualisez les r√©sultats avec: trackio show\")\n",
    "print(\"   - Ou consultez le dashboard TrackIO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Pour aller plus loin\n",
    "\n",
    "### Exp√©rimentations sugg√©r√©es\n",
    "\n",
    "1. **Variation du rank LoRA**: Tester r=8, r=16, r=32 et comparer les performances\n",
    "2. **Diff√©rents taux d'apprentissage**: 1e-4, 2e-4, 5e-4\n",
    "3. **Augmentation du dataset**: G√©n√©rer plus de donn√©es synth√©tiques\n",
    "\n",
    "### Optimisations possibles\n",
    "\n",
    "1. **Batch processing**: Augmenter la taille du batch si m√©moire permet\n",
    "2. **Gradient checkpointing**: Activ√© par d√©faut dans Unsloth\n",
    "3. **Mixed precision**: Utiliser bf16 si disponible\n",
    "4. **Data parallelism**: Pour multi-GPU\n",
    "\n",
    "### √âvaluation approfondie\n",
    "\n",
    "1. **Benchmarks automatiques**: Utiliser des benchmarks fran√ßais\n",
    "2. **√âvaluation humaine**: Qualit√© des r√©ponses g√©n√©r√©es\n",
    "3. **Comparaison baseline**: Contre le mod√®le de base\n",
    "4. **Analyse d'erreurs**: Comprendre les faiblesses du mod√®le\n",
    "\n",
    "### D√©ploiement\n",
    "\n",
    "2. **Optimisation inf√©rence**: Regarder vLLM et SGLang\n",
    "3. **Quantification post-entra√Ænement**: GGUF, AWQ\n",
    "\n",
    "### TrackIO avanc√©\n",
    "\n",
    "Pour analyser vos exp√©riences plus en d√©tail:\n",
    "\n",
    "```python\n",
    "# Comparer plusieurs exp√©riences\n",
    "trackio.compare_experiments([\n",
    "    \"qwen3-4b-lora-french-1\",\n",
    "    \"qwen3-4b-lora-french-2\",\n",
    "    \"qwen3-4b-lora-french-3\"\n",
    "])\n",
    "\n",
    "# Exporter les r√©sultats\n",
    "trackio.export_results(\"experiment_results.csv\")\n",
    "\n",
    "# Visualiser les courbes d'apprentissage\n",
    "trackio.plot_training_curves()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
