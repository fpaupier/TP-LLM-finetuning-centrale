{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Partie 1 : G√©n√©ration de Dataset Synth√©tique pour le Fine-tuning de LLM\n",
    "\n",
    "## üìö Contexte\n",
    "\n",
    "Dans ce notebook, nous allons explorer la g√©n√©ration de datasets d'instruction synth√©tiques, une technique cl√© pour cr√©er des donn√©es d'entra√Ænement pour le fine-tuning de mod√®les de langage. Cette approche permet de :\n",
    "\n",
    "- **Cr√©er des donn√©es d'entra√Ænement** sans annotation manuelle co√ªteuse\n",
    "- **Adapter un mod√®le** √† des domaines sp√©cifiques\n",
    "- **Augmenter la diversit√©** des donn√©es existantes\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "1. **Comprendre** le processus de g√©n√©ration de donn√©es synth√©tiques\n",
    "2. **Impl√©menter** une pipeline de g√©n√©ration bas√©e sur des documents sources\n",
    "3. **Cr√©er** un dataset au format instruction-response utilisable pour le fine-tuning\n",
    "4. **Analyser** la qualit√© et la diversit√© des donn√©es g√©n√©r√©es\n",
    "\n",
    "## üîß Architecture\n",
    "\n",
    "Notre approche s'inspire de la m√©thodologie AWS pour la g√©n√©ration de donn√©es synth√©tiques :\n",
    "\n",
    "1. **Extraction de contexte** : Nous utilisons le corpus Common Corpus (split fran√ßais)\n",
    "2. **G√©n√©ration de questions** : Un LLM g√©n√®re des questions pertinentes bas√©es sur le contexte\n",
    "3. **G√©n√©ration de r√©ponses** : Le m√™me ou un autre LLM g√©n√®re des r√©ponses aux questions\n",
    "4. **Formatage** : Structuration au format instruction-suivie pour le fine-tuning\n",
    "\n",
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Nous utilisons :\n",
    "- **vLLM** : Serveur d'inf√©rence haute performance (d√©j√† configur√© sur votre machine)\n",
    "- **Qwen3-4B** : Mod√®le de base pour la g√©n√©ration\n",
    "- **Common Corpus** : Dataset source en fran√ßais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import des biblioth√®ques et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration visuelle\n",
    "sns.set_theme()\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration du client vLLM\n",
    "\n",
    "**Note importante** : Assurez-vous que votre serveur vLLM est lanc√© (`make vllm` dans un terminal s√©par√©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du client OpenAI pour vLLM\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"ecn-llm-token-update-this-secret\"  # Token d√©fini dans start_vllm.sh\n",
    ")\n",
    "\n",
    "# Test de connexion\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    print(\"‚úÖ Connexion au serveur vLLM r√©ussie\")\n",
    "    print(f\"Mod√®le disponible : {models.data[0].id if models.data else 'Aucun'}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de connexion : {e}\")\n",
    "    print(\"Assurez-vous d'avoir lanc√© le serveur vLLM avec 'make vllm'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement du dataset source\n",
    "\n",
    "Nous utilisons le **Common Corpus** en fran√ßais comme base pour g√©n√©rer nos instructions synth√©tiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset\n",
    "print(\"Chargement du dataset Common Corpus (split fran√ßais)...\")\n",
    "dataset = load_dataset(\n",
    "    \"PleIAs/common_corpus\",\n",
    "    split=\"train\",\n",
    "    streaming=True  # Streaming pour √©conomiser la m√©moire\n",
    ")\n",
    "\n",
    "# √âchantillonnage pour le TP (ajustez selon vos besoins)\n",
    "SAMPLE_SIZE = 100  # Nombre de documents √† traiter\n",
    "documents = []\n",
    "\n",
    "print(f\"Extraction de {SAMPLE_SIZE} documents...\")\n",
    "for i, doc in enumerate(tqdm(dataset, total=SAMPLE_SIZE)):\n",
    "    if i >= SAMPLE_SIZE:\n",
    "        break\n",
    "    # Filtrer les documents trop courts ou trop longs\n",
    "    if doc.get('text') and 100 < len(doc['text']) < 2000:\n",
    "        documents.append(doc['text'])\n",
    "\n",
    "print(f\"\\nüìä Documents collect√©s : {len(documents)}\")\n",
    "print(f\"Longueur moyenne : {np.mean([len(d) for d in documents]):.0f} caract√®res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. D√©finition des structures de donn√©es\n",
    "\n",
    "Nous utilisons des dataclasses pour structurer nos donn√©es de mani√®re claire et typ√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InstructionPair:\n",
    "    \"\"\"Structure pour une paire instruction-r√©ponse\"\"\"\n",
    "    instruction: str\n",
    "    response: str\n",
    "    context: str\n",
    "    metadata: Dict = None\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "    \n",
    "    def to_alpaca_format(self) -> Dict:\n",
    "        \"\"\"Conversion au format Alpaca standard\"\"\"\n",
    "        return {\n",
    "            \"instruction\": self.instruction,\n",
    "            \"input\": \"\",  # Contexte optionnel\n",
    "            \"output\": self.response\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline de g√©n√©ration synth√©tique\n",
    "\n",
    "### 5.1 Prompts pour la g√©n√©ration\n",
    "\n",
    "Nous d√©finissons des prompts soigneusement con√ßus pour g√©n√©rer des questions et r√©ponses de qualit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates de prompts pour la g√©n√©ration\n",
    "QUESTION_GENERATION_PROMPT = \"\"\"Tu es un assistant p√©dagogique expert. √Ä partir du texte suivant, g√©n√®re {num_questions} questions pertinentes et vari√©es en fran√ßais.\n",
    "\n",
    "Les questions doivent :\n",
    "- √ätre claires et pr√©cises\n",
    "- Couvrir diff√©rents aspects du texte\n",
    "- Varier en complexit√© (compr√©hension, analyse, synth√®se)\n",
    "- √ätre formul√©es en fran√ßais correct\n",
    "\n",
    "Texte source :\n",
    "{context}\n",
    "\n",
    "G√©n√®re les questions au format JSON :\n",
    "[\"question1\", \"question2\", ...]\n",
    "\"\"\"\n",
    "\n",
    "ANSWER_GENERATION_PROMPT = \"\"\"Tu es un assistant expert. R√©ponds √† la question suivante de mani√®re claire, pr√©cise et p√©dagogique en te basant sur le contexte fourni.\n",
    "\n",
    "Contexte :\n",
    "{context}\n",
    "\n",
    "Question :\n",
    "{question}\n",
    "\n",
    "R√©ponse :\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Fonctions de g√©n√©ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(context: str, num_questions: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    G√©n√®re des questions bas√©es sur un contexte donn√©.\n",
    "    \n",
    "    Args:\n",
    "        context: Le texte source\n",
    "        num_questions: Nombre de questions √† g√©n√©rer\n",
    "    \n",
    "    Returns:\n",
    "        Liste de questions g√©n√©r√©es\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"Qwen/Qwen3-4B\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Tu es un assistant qui g√©n√®re des questions p√©dagogiques.\"},\n",
    "                {\"role\": \"user\", \"content\": QUESTION_GENERATION_PROMPT.format(\n",
    "                    context=context[:1500],  # Limiter la longueur du contexte\n",
    "                    num_questions=num_questions\n",
    "                )}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        # Extraction des questions du JSON\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Tentative de parsing JSON\n",
    "        try:\n",
    "            # Chercher le JSON dans la r√©ponse\n",
    "            import re\n",
    "            json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                questions = json.loads(json_match.group())\n",
    "                return questions[:num_questions]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Fallback : extraction simple des questions\n",
    "        lines = content.split('\\n')\n",
    "        questions = [line.strip('- ').strip() for line in lines \n",
    "                    if line.strip() and '?' in line]\n",
    "        return questions[:num_questions] if questions else [\"Quelle est l'id√©e principale de ce texte ?\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la g√©n√©ration de questions : {e}\")\n",
    "        return [\"Quelle est l'id√©e principale de ce texte ?\"]\n",
    "\n",
    "\n",
    "def generate_answer(context: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    G√©n√®re une r√©ponse √† une question bas√©e sur un contexte.\n",
    "    \n",
    "    Args:\n",
    "        context: Le texte source\n",
    "        question: La question √† r√©pondre\n",
    "    \n",
    "    Returns:\n",
    "        R√©ponse g√©n√©r√©e\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"Qwen/Qwen3-4B\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Tu es un assistant expert qui r√©pond de mani√®re claire et p√©dagogique.\"},\n",
    "                {\"role\": \"user\", \"content\": ANSWER_GENERATION_PROMPT.format(\n",
    "                    context=context[:1500],\n",
    "                    question=question\n",
    "                )}\n",
    "            ],\n",
    "            temperature=0.3,  # Temp√©rature plus basse pour des r√©ponses coh√©rentes\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la g√©n√©ration de r√©ponse : {e}\")\n",
    "        return \"Je ne peux pas r√©pondre √† cette question bas√©e sur le contexte fourni.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Pipeline compl√®te de g√©n√©ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instruction_pairs(\n",
    "    documents: List[str], \n",
    "    questions_per_doc: int = 2,\n",
    "    max_docs: Optional[int] = None\n",
    ") -> List[InstructionPair]:\n",
    "    \"\"\"\n",
    "    G√©n√®re des paires instruction-r√©ponse √† partir de documents.\n",
    "    \n",
    "    Args:\n",
    "        documents: Liste de documents sources\n",
    "        questions_per_doc: Nombre de questions par document\n",
    "        max_docs: Nombre maximum de documents √† traiter\n",
    "    \n",
    "    Returns:\n",
    "        Liste de paires instruction-r√©ponse\n",
    "    \"\"\"\n",
    "    instruction_pairs = []\n",
    "    docs_to_process = documents[:max_docs] if max_docs else documents\n",
    "    \n",
    "    for doc in tqdm(docs_to_process, desc=\"Traitement des documents\"):\n",
    "        # G√©n√©rer les questions\n",
    "        questions = generate_questions(doc, questions_per_doc)\n",
    "        \n",
    "        # G√©n√©rer les r√©ponses pour chaque question\n",
    "        for question in questions:\n",
    "            if question and len(question) > 10:  # Filtre de qualit√© basique\n",
    "                answer = generate_answer(doc, question)\n",
    "                \n",
    "                if answer and len(answer) > 20:  # Filtre de qualit√© pour la r√©ponse\n",
    "                    pair = InstructionPair(\n",
    "                        instruction=question,\n",
    "                        response=answer,\n",
    "                        context=doc[:500],  # Garder un extrait du contexte\n",
    "                        metadata={\n",
    "                            \"source\": \"common_corpus_fr\",\n",
    "                            \"generation_method\": \"vllm_qwen3_4b\"\n",
    "                        }\n",
    "                    )\n",
    "                    instruction_pairs.append(pair)\n",
    "    \n",
    "    return instruction_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. G√©n√©ration du dataset synth√©tique\n",
    "\n",
    "**‚ö†Ô∏è Note** : Cette √©tape peut prendre plusieurs minutes selon le nombre de documents et la vitesse de votre GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres de g√©n√©ration\n",
    "NUM_DOCS_TO_PROCESS = 10  # Commencez petit pour tester\n",
    "QUESTIONS_PER_DOC = 2\n",
    "\n",
    "print(f\"üöÄ D√©marrage de la g√©n√©ration synth√©tique\")\n",
    "print(f\"   - Documents √† traiter : {NUM_DOCS_TO_PROCESS}\")\n",
    "print(f\"   - Questions par document : {QUESTIONS_PER_DOC}\")\n",
    "print(f\"   - Paires attendues : ~{NUM_DOCS_TO_PROCESS * QUESTIONS_PER_DOC}\\n\")\n",
    "\n",
    "# G√©n√©ration\n",
    "synthetic_pairs = generate_instruction_pairs(\n",
    "    documents,\n",
    "    questions_per_doc=QUESTIONS_PER_DOC,\n",
    "    max_docs=NUM_DOCS_TO_PROCESS\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ G√©n√©ration termin√©e : {len(synthetic_pairs)} paires cr√©√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyse et visualisation du dataset g√©n√©r√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en DataFrame pour l'analyse\n",
    "df = pd.DataFrame([pair.to_dict() for pair in synthetic_pairs])\n",
    "\n",
    "# Statistiques de base\n",
    "print(\"üìä Statistiques du dataset g√©n√©r√© :\")\n",
    "print(f\"   - Nombre total de paires : {len(df)}\")\n",
    "print(f\"   - Longueur moyenne des instructions : {df['instruction'].str.len().mean():.0f} caract√®res\")\n",
    "print(f\"   - Longueur moyenne des r√©ponses : {df['response'].str.len().mean():.0f} caract√®res\")\n",
    "\n",
    "# Affichage d'exemples\n",
    "print(\"\\nüìù Exemples de paires g√©n√©r√©es :\\n\")\n",
    "for i in range(min(3, len(synthetic_pairs))):\n",
    "    print(f\"--- Exemple {i+1} ---\")\n",
    "    print(f\"Instruction : {synthetic_pairs[i].instruction}\")\n",
    "    print(f\"R√©ponse : {synthetic_pairs[i].response[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution des longueurs d'instructions\n",
    "axes[0].hist(df['instruction'].str.len(), bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Longueur (caract√®res)')\n",
    "axes[0].set_ylabel('Fr√©quence')\n",
    "axes[0].set_title('Distribution des longueurs d\\'instructions')\n",
    "axes[0].axvline(df['instruction'].str.len().mean(), color='red', \n",
    "                linestyle='--', label=f'Moyenne: {df[\"instruction\"].str.len().mean():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Distribution des longueurs de r√©ponses\n",
    "axes[1].hist(df['response'].str.len(), bins=20, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('Longueur (caract√®res)')\n",
    "axes[1].set_ylabel('Fr√©quence')\n",
    "axes[1].set_title('Distribution des longueurs de r√©ponses')\n",
    "axes[1].axvline(df['response'].str.len().mean(), color='red', \n",
    "                linestyle='--', label=f'Moyenne: {df[\"response\"].str.len().mean():.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde du dataset\n",
    "\n",
    "Nous sauvegardons le dataset dans plusieurs formats pour faciliter son utilisation ult√©rieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Cr√©ation du dossier de sortie\n",
    "output_dir = Path(\"../data/synthetic\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Format Alpaca (standard pour le fine-tuning)\n",
    "alpaca_data = [pair.to_alpaca_format() for pair in synthetic_pairs]\n",
    "alpaca_path = output_dir / \"synthetic_dataset_alpaca.json\"\n",
    "with open(alpaca_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(alpaca_data, f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úÖ Dataset au format Alpaca sauvegard√© : {alpaca_path}\")\n",
    "\n",
    "# Format CSV pour l'analyse\n",
    "csv_path = output_dir / \"synthetic_dataset.csv\"\n",
    "df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "print(f\"‚úÖ Dataset CSV sauvegard√© : {csv_path}\")\n",
    "\n",
    "# Format JSONL (ligne par ligne, efficace pour le streaming)\n",
    "jsonl_path = output_dir / \"synthetic_dataset.jsonl\"\n",
    "with open(jsonl_path, 'w', encoding='utf-8') as f:\n",
    "    for pair in synthetic_pairs:\n",
    "        f.write(json.dumps(pair.to_dict(), ensure_ascii=False) + '\\n')\n",
    "print(f\"‚úÖ Dataset JSONL sauvegard√© : {jsonl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Contr√¥le qualit√© et filtrage\n",
    "\n",
    "Il est important de v√©rifier la qualit√© des donn√©es g√©n√©r√©es avant de les utiliser pour le fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_check(pair: InstructionPair) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    V√©rifie la qualit√© d'une paire instruction-r√©ponse.\n",
    "    \n",
    "    Returns:\n",
    "        (pass_check, list_of_issues)\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # V√©rifications de base\n",
    "    if len(pair.instruction) < 10:\n",
    "        issues.append(\"Instruction trop courte\")\n",
    "    if len(pair.response) < 20:\n",
    "        issues.append(\"R√©ponse trop courte\")\n",
    "    if len(pair.instruction) > 500:\n",
    "        issues.append(\"Instruction trop longue\")\n",
    "    if len(pair.response) > 1000:\n",
    "        issues.append(\"R√©ponse trop longue\")\n",
    "    \n",
    "    # V√©rification de la pr√©sence de caract√®res sp√©ciaux probl√©matiques\n",
    "    if any(char in pair.instruction + pair.response for char in ['\\x00', '\\r']):\n",
    "        issues.append(\"Caract√®res sp√©ciaux d√©tect√©s\")\n",
    "    \n",
    "    # V√©rification de la coh√©rence\n",
    "    if '?' not in pair.instruction:\n",
    "        issues.append(\"L'instruction ne contient pas de question\")\n",
    "    \n",
    "    return len(issues) == 0, issues\n",
    "\n",
    "# Application du contr√¥le qualit√©\n",
    "quality_results = []\n",
    "for pair in synthetic_pairs:\n",
    "    passed, issues = quality_check(pair)\n",
    "    quality_results.append({\n",
    "        'passed': passed,\n",
    "        'issues': issues,\n",
    "        'pair': pair\n",
    "    })\n",
    "\n",
    "# Statistiques de qualit√©\n",
    "passed_count = sum(1 for r in quality_results if r['passed'])\n",
    "print(f\"üìä R√©sultats du contr√¥le qualit√© :\")\n",
    "print(f\"   - Paires valid√©es : {passed_count}/{len(quality_results)} ({100*passed_count/len(quality_results):.1f}%)\")\n",
    "\n",
    "# Affichage des probl√®mes les plus fr√©quents\n",
    "all_issues = [issue for r in quality_results for issue in r['issues']]\n",
    "if all_issues:\n",
    "    from collections import Counter\n",
    "    issue_counts = Counter(all_issues)\n",
    "    print(\"\\nüîç Probl√®mes d√©tect√©s :\")\n",
    "    for issue, count in issue_counts.most_common(5):\n",
    "        print(f\"   - {issue}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du dataset filtr√©\n",
    "filtered_pairs = [r['pair'] for r in quality_results if r['passed']]\n",
    "\n",
    "if filtered_pairs:\n",
    "    filtered_path = output_dir / \"synthetic_dataset_filtered.json\"\n",
    "    with open(filtered_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(\n",
    "            [pair.to_alpaca_format() for pair in filtered_pairs],\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2\n",
    "        )\n",
    "    print(f\"\\n‚úÖ Dataset filtr√© sauvegard√© : {filtered_path}\")\n",
    "    print(f\"   Contient {len(filtered_pairs)} paires de haute qualit√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pr√©paration pour le fine-tuning\n",
    "\n",
    "### Recommandations pour l'√©tape suivante\n",
    "\n",
    "Votre dataset synth√©tique est maintenant pr√™t pour le fine-tuning ! Voici quelques recommandations :\n",
    "\n",
    "1. **Taille du dataset** : \n",
    "   - Minimum recommand√© : 100-500 paires pour un fine-tuning basique\n",
    "   - Id√©al : 1000-5000 paires pour de meilleurs r√©sultats\n",
    "\n",
    "2. **Diversit√©** :\n",
    "   - Variez les types de questions (factuelle, analytique, cr√©ative)\n",
    "   - Utilisez diff√©rents domaines de contexte\n",
    "\n",
    "3. **Qualit√©** :\n",
    "   - Privil√©giez la qualit√© √† la quantit√©\n",
    "   - V√©rifiez manuellement un √©chantillon\n",
    "\n",
    "4. **Format** :\n",
    "   - Le format Alpaca est directement utilisable avec la plupart des frameworks\n",
    "   - Gardez une trace du contexte original pour l'√©valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© final\n",
    "print(\"üéâ G√©n√©ration de dataset synth√©tique termin√©e !\\n\")\n",
    "print(\"üìä R√©sum√© :\")\n",
    "print(f\"   - Documents sources trait√©s : {NUM_DOCS_TO_PROCESS}\")\n",
    "print(f\"   - Paires g√©n√©r√©es : {len(synthetic_pairs)}\")\n",
    "print(f\"   - Paires de haute qualit√© : {len(filtered_pairs) if filtered_pairs else 0}\")\n",
    "print(f\"\\nüìÅ Fichiers cr√©√©s :\")\n",
    "print(f\"   - {alpaca_path.name} (format Alpaca pour fine-tuning)\")\n",
    "print(f\"   - {csv_path.name} (format CSV pour analyse)\")\n",
    "print(f\"   - {jsonl_path.name} (format JSONL pour streaming)\")\n",
    "if filtered_pairs:\n",
    "    print(f\"   - {filtered_path.name} (dataset filtr√©)\")\n",
    "\n",
    "print(\"\\nüöÄ Prochaine √©tape : Notebook 02 - Fine-tuning avec LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Pour aller plus loin\n",
    "\n",
    "### Am√©liorations possibles\n",
    "\n",
    "1. **Diversification des prompts** :\n",
    "   - Cr√©er plusieurs templates de prompts\n",
    "   - Adapter les prompts selon le type de contenu\n",
    "\n",
    "2. **Techniques avanc√©es** :\n",
    "   - Self-instruct : Utiliser le mod√®le pour g√©n√©rer ses propres instructions\n",
    "   - Constitutional AI : Ajouter des contraintes √©thiques\n",
    "   - √âvolution : Faire √©voluer les instructions progressivement\n",
    "\n",
    "3. **Validation** :\n",
    "   - Utiliser un mod√®le diff√©rent pour valider les r√©ponses\n",
    "   - Impl√©menter un syst√®me de scoring automatique\n",
    "   - Cross-validation avec des annotateurs humains\n",
    "\n",
    "4. **Optimisation** :\n",
    "   - Batch processing pour acc√©l√©rer la g√©n√©ration\n",
    "   - Caching des r√©sultats interm√©diaires\n",
    "   - Parall√©lisation des requ√™tes\n",
    "\n",
    "### Exercices propos√©s\n",
    "\n",
    "1. **Modifier les prompts** pour g√©n√©rer des instructions dans un domaine sp√©cifique (m√©dical, juridique, technique)\n",
    "2. **Impl√©menter un syst√®me de scoring** pour √©valuer automatiquement la qualit√© des paires\n",
    "3. **Cr√©er une pipeline de validation** avec un second mod√®le\n",
    "4. **Exp√©rimenter avec diff√©rentes temp√©ratures** et analyser l'impact sur la diversit√©"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}